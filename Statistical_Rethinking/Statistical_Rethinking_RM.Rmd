---
title: "Statistical_Rethinking_RM"
author: "SM"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    toc: true 
    toc_float: true
    toc_depth: 3  
    number_sections: true  
    theme: united  
    highlight: tango  
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rethinking)
library(ggplot2)
library(dplyr)
library(patchwork)
library(MASS)
```

# Chapter 1: The Golem of Prague

$~$

Chapter 1 urged us to rethink the "popular statistical and scientific philosophy" by pointing out the flaws in classical statistical tools and reasoning. 

According to the author, "classical tools are not diverse enough to handle many common research questions...And if we keep adding new types of tools, soon there will be far too many to keep track of." I agree with these statements. The author also talked about how Fisher's exact test is used "whenever the cell counts are small" and he had "never seen it used appropriately" except Fisher's original use of the test. It would be nice if the author gave some examples and explanations of those inappropriate uses- I can't decide whether to accept without more information. 

The author mentioned why deductive falsification is impossible in scientific context:

(1) "Hypotheses are not models. The relations among hypotheses and different kinds of models are complex. Many models correspond to the same hypothesis, and many hypotheses correspond to a single model. This makes strict falsification impossible.

(2) Measurement matters. Even when we think the data falsify a model, another observer will debate our methods and measures. They don’t trust the data. Sometimes they are right.

For both of these reasons, deductive falsification never works. The scientific method cannot be reduced to a statistical procedure, and so our statistical methods should not pretend." 

$~$

## Hypotheses are not models

He gave an example of the first statement in evolutionary context. His conclusion was:

(1) "Any given statistical model (M) may correspond to more than one process model (P).

(2) Any given hypothesis (H) may correspond to more than one process model (P). 

(3) Any given statistical model (M) may correspond to more than one hypothesis (H)."

In other words, it doesn't matter whether we reject or fail to reject the null. It won't have "much inferential power" or give any useful insights due to possible existence of models- other than the ones under study- which can also align with the null or alternative hypotheses. The author gave a solution to this dilemma- "explicitly compare predictions of more than one model." This is a very nice solution, as I think it will allow analysis of hypothesis or whatever you want to study in real-world conditions. And that is much more powerful than any classical tests. 

In that case, should we disregard all studies that rely on NHST? Should we reject all findings made by rejecting the null hypothesis as senseless? And are the multiple process models the only way to go? The frequentists' way to solve this problem is to control the conditions of the experiment/study. In nutrition studies, this can be in the form of inclusion or exclusion criteria and strict control of test subjects (and conditions) throughout the duration of the study. This is to ensure the observed effect (or lack of) is caused by (or related to) the variables of interest and not by uncontrolled confounders. If a NHST research was done well with proper control, then we could 'more or less' have confidence in the research's findings. 

$~$

## *Measurement matters*

In the section "measurement matters," the author reinforced the first statement, and explained the fallacy of falsification principle, stating two reasons:

1. "Observations are prone to error, especially at the boundaries of scientific knowledge. 

2. Most hypotheses are quantitative, concerning the degrees of existence, rather than discrete, concerning total presence or absence."

### *Observation error*

The author gave two examples of observation error. The first example pointed out possible bias or errors in observations, and potential type 1 and type 2 errors (false positives and false negatives) in classical frequentist researches. And the second example talked about errors in measurements (or studies/ experiments). Both are valid concerns and those errors are not uncommon in frequentist researches. 

There are several ways to overcome these shortcomings, depending on research design. For example, we can increase sample size or run more tests to reduce type 1 and type 2 errors. Errors in measurements can be avoided by following protocols- taking at least two measurements for each sample, consistency in measurements, etc. However, there is no guarantee that any particular researcher or research group will follow a strict protocol without compromising, and that can lead to observation error. To prevent this, we usually avoid using finding from a single research, and wait till the findings are backed by systematic reviews and made into guidelines. 

In both woodpecker and neutrino examples, the author's key concerns were "whether falsification is real or spurious," and that "both true-detection and false-detection plausible." The null hypothesis in the woodpecker example was "The Ivory-billed Woodpecker is extinct." According to the author, it will only take a single woodpecker to falsify this theory, and that the 2005 evidence on existence of a woodpecker was doubted by many. Since no other evidences emerged, the question remained unresolved. That means finding was not conclusive yet, and research was still ongoing. As I mentioned above, we don't accept findings from a single research as concrete or irrefutable evidence. Thus, the dilemma over falsification and type 1 or type 2 error in the woodpecker case was not really a concern- we would just need to conduct more surveys (wider location for longer period?) or simply wait for new evidences. However, I think this is not a good case to use the null hypothesis testing. The falsification of hypothesis in this case will be simply the result of patience or luck, rather than proper research design or statistical tests. In the neutrino example, the problem was the human error which they solved by replication of experiments. This is not unusual in frequentist research. Replicating or reconducting experiment is a common occurrence, especially when lab analyses are involved. It may be an inconvenience, but it does not imply the utter failure of the theory of falsification or the null hypothesis testing. 

### *Continuous hypotheses*

The author noted "...it is a good practice to design experiments and observations that can differentiate competing hypotheses. But in many cases, the comparison must be probabilistic, a matter of degree, not kind." Two examples of null hypotheses that support the above statement were given: "80% of swans are white" and "Black swans are rare." The author pointed out the *modus tollens* swan story would not be applicable in this case, since the problem is not to prove or disprove the hypothesis but to estimate the distribution of swan coloration. He added "You might object that the hypothesis above is just not a good scientific hypothesis, because it isn’t easy to disprove. But if that’s the case, then most of the important questions about the world are not good scientific hypotheses."

In my opinion, it is not difficult to disprove both hypotheses. The two hypothese are not good hypotheses simply because they are not specific and fail to form sensible research question or outcome- not because it is "a matter of degree." Let's break down the two hypotheses. First, we think about the possible research question for the hypothesis "80% of swan are white." The research question could be "Are 80% of swan white?" or "Do 80% of swan have white feathers?" In this case, this hypothesis would be disprove if survey found any percent of white swan other than 80%. It wouldn't matter if the actual percent was 90 or 95- we could stop the research once the number passed the threshold of 80%. Now it is clear that the problem with this hypothesis is the pointless outcome. What are we trying to achieve knowing whether 80% of swan are white, or 40% or any percentage of swan are white in color? A more sensible hypothesis would be "White coloration is the dominant trait in swan." We can falsify this hypothesis by conducting surveys- like the author suggested- or genetic analysis, etc. Still, it is not a good hypothesis for an original research since the scope is too broad. The hypothesis only specify swan- not a particular speices or geographical location, meaning all swan species in the world are included in the research. That would be a hard task for any single researcher or research group. Hence, this hypothesis is more suitable at review level- not original research level. For original research, we can specify the species of swan and extent of study area based on the available funding.

The second hypothesis "Black swans are rare" has the similar problem- lack of specificity. We need to determine what is considered rare- 1 in 100 or 1 in a million? Once we have that specification, it is not difficult to tackle the second hypothesis. But we need to be aware that the second hypothesis also includes all swan species in the world- or even universe or parallel universes. 

Another thing is the *modus tollens* swan story was merely to explain a theory- not a definitive example of how we should write hypothesis. I have yet to see a research with hypothesis written like the one in *modus tollens* swan story. All hypotheses that I have seen so far address "the degree, not kind"-like the author suggested. Hence, the author's concern over hypotheses is somehow invalid. 

### *Falsification is consensual*

The author said "falsification is always consensual, not logical," and "evidence often— but not always— has something to do with such falsification." He also highlighted the arguments towards "consensus about meaning of evidence" and the danger of "historical revisionism." I think the author is right. We hold such reverence towards the famous scientists in history, that their formulas are still widely applied to this day, without contesting their suitability in this age. Many researches were done following the same mould, that they gave similar results, which became our gold-standard- the guidelines. And those guidelines are not set in stone. They are regularly updated- albeit minor changes. Occasionally, some guidelines are completely proven false and replaced by new ones. These occasions are rare that they are called groundbreaking discovery. I remembered the day everyone at the department was talking about the guideline change on egg consumption- may seem insignificant to outsiders but we were really excited! It could be that such new discoveries are rare because we are doing the same thing over and over again, and are content when our findings (even vaguely) agree the current consensus- we seek validation from previous works by researchers more venerable than us. And that is dangerous to our patients and the public. We may be feeling relaxed because we think the worst that can happen from such findings is "the treatment is ineffective but not harmful." We pour scorn on some of the treatments given by the healthcare providers some hundred years ago as harmful and ineffective. We can do so because we now have the knowledge collected over time. It is very possible our best guidelines may seem 'folly' to the researchers in the next hundred years. Thus, I agree with the author that it's time we should do things differently. 

$~$

## Three tools for golem engineering 


The author gave alternative to falsification- that is to build models. According to the author, "Models can be made into testing procedures— all statistical tests are also models—but they can also be used to measure, forecast, and argue." He also mentioned three models that are "in demand in both the social and biological sciences."

(1) Bayesian data analysis

(2) Multilevel models

(3) Model comparison using information criteria


### *Bayesian data analysis*

As explained by the author, Bayesian data analysis uses uncertainty or chance to "describe plausibility of different possibilities," and "...we can use probability theory as a general way to represent plausibility, whether in reference to countable events in the world or rather theoretical constructs like parameters." The author then compared this approach to frequentist approach- "Bayesian probability is a very general approach to probability...The frequentist approach requires that all probabilities be defined by connection to countable events and their frequencies in very large samples." And the author pointed out "...This leads to frequentist uncertainty being premised on imaginary resampling of data— if we were to repeat the measurement many many times, we would end up collecting a list of values that will have some pattern to it. It means also that parameters and models cannot have probability distributions, only measurements can. The distribution of these measurements is called a sampling distribution. This resampling is never done, and in general it doesn’t even make sense..." 

He also quoted "one of the most important frequentist statisticians of the 20^th^ century," Sir Ronald Fisher:

"[...] the only populations that can be referred to in a test of significance have no objective reality, being exclusively the product of the statistician’s imagination [...]"

But, Fisher's criticism was already rebuked by his contemporary Professor Egon Pearson in his ["Statistical Concepts in Their Relation to Reality"](https://errorstatistics.files.wordpress.com/2021/02/pearson_1955-stat-concepts-reality.pdf). Pearson said:

"...If probability is to be justly applied to the analysis of data, it follows that a random process must have been introduced or been naturally present at some stage in the collection of these data. Is there not then an appeal to the imagination in taking as the hypothetical population of samples that which would have been generated by repetition of this random process?..."

According to Pearson, Fisher was "often tilting at views which those whom he attacks have never held." And I can use Pearson's argument to address the "telescope" example given by the author.

"...Since the telescope was primitive, it couldn’t really focus the image very well. Saturn always appeared blurred. This is a statistical problem, of a sort. There’s uncertainty about the planet’s shape, but notice that none of the uncertainty is a result of variation in repeat measurements. We could look through the telescope a thousand times, and it will always give the same blurred image (for any given position of the Earth and Saturn). So the sampling distribution of any measurement is constant, because the measurement is deterministic— there’s nothing “random” about it. Frequentist statistical inference has a lot of trouble getting started here..."

I think, like Fisher, the author is also "tilting at views" which frequentists have never held. The problem with the telescope example is that repeated measurements were done on "ONE" sample only. That's why he made assumption that "...none of the uncertainty is a result of variation in repeat measurements...So the sampling distribution of any measurement is constant, because the measurement is deterministic— there’s nothing “random” about it." I have yet to see a research that only measures "one" sample repeatedly to solve a research question at population level or in real-world situations. It would be akin to asking a single child- using different sets of questions- repeatedly about what the child eats in a day to know the dietary pattern of all the children in the region. It makes no sense. Furthermore, sample representativeness play a major role in such researches. The author representation of a frequentist research has no place for sample representativeness- there is only one sample and nothing else. 

The author also criticized about "imagined repeat sampling." If taken out of context, such idea can be quite preposterous. But, it can actually be logical if we explain it in terms of sample representativeness. We are not imagining repeated sampling- beyond our chosen sample sets- to justify the results from our samples are applicable to the population. For example, if we were to study the dietary pattern of an ethnic group reside in the mountain, we do not need to ask dietary-related questions to every individual in the group. We select a fraction of the group- chosen at random- to infer the dietary pattern of the whole group. Such fraction (samples) should be high enough in number and have traits similar to the whole group. If results obtained from samples said "fish constitutes only a small part of their diet," then we can assume that we will have the same result if we did the same survey repeatedly with different sample sets obtained from the group. In this case, the so-called "imagined repeat sampling" can work because samples are representative of the population.  

The author continued in the next paragraph that "..even when a Bayesian procedure and frequentist procedure give exactly the same answer, our Bayesian golems aren’t justifying their inferences with imagined repeat sampling." Such narrative can give a result totally opposite of what the author wants. The author wants the readers to "rethinking statistics," by pointing out flaws in the classical statistic tests and research methodology. However, all the examples given by the author can be easily rebuked. More rebuttals will come if such narrative is introduced in a frequestist community- like academics in the nutrition field. But, these academics are not really protecting Karl Popper or E.S. Pearson. They are not defensive about the theories postulated years ago by scientists in the past. They are simply protecting their works- their findings. Majority- if not all- of these academics will not "rethink statistics" if Bayesian approach begins with denouncing the frequentist approach, especially when counter-arguments can easily be made. It will be similar to the author stabbing them with a knife first before offering a treatment. All or most of them will not accept the treatment. They will run away- either scared or hateful. Some may even stab him back. Even if they were forced to accept it, the acceptance would be superficial. Instead, we should offer them beautiful roses with exquisite scent- meaning we should focus on what Bayesian analysis can offer which frequentist apporach can't, rather than attacking them. The thorns may still prick their fingers, but they will not be able to resist the charm of the roses. And we can offer them balm to soothe their fingers. They will be thankful, and meaningful collaborations will be made.   

### *Multilevel models*

The author introduced the concept of multilevel model in this section. Statistical models "contain parameters. And parameters support inference. Upon what do parameters themselves stand? Sometimes, in some of the most powerful models, it’s parameters all the way down. What this means is that any particular parameter can be usefully regarded as a placeholder for a missing model. Given some model of how the parameter gets its value, it is simple enough to embed the new model inside the old one. This results in a model with multiple levels of uncertainty, each feeding into the next—a multilevel model."

The author also mentioned "four typical and complementary reasons to sue multilevel models:"

(1) ***To adjust estimates for repeat sampling.*** When more than one observation arises from the same individual, location, or time, then traditional, single-level models may mislead us.

(2) ***To adjust estimates for imbalance in sampling.*** When some individuals,locations,or times are sampled more than others, we may also be misled by single-level models.

(3) ***To study variation.*** If our research questions include variation among individuals or other groups within the data, then multilevel models are a big help, because they model variation explicitly.

(4) ***To avoid averaging.*** Frequently, scholars pre-average some data to construct variables for a regression analysis. This can be dangerous, because averaging removes variation. It therefore manufactures false confidence. Multilevel models allow us to preserve the uncertainty in the original, pre-averaged values, while still using the average to make predictions.

And there are also "diverse model types that turn out to be multilevel: models for missing data (imputation), measurement error, factor analysis, some time series models, types of spatial and network regression, and phylogenetic regressions all are special applications of the multilevel strategy."

The author added "this is why grasping the concept of multilevel modeling may lead to a perspective shift. Suddenly single- level models end up looking like mere components of multilevel models. The multilevel strategy provides an engineering principle to help us to introduce these components into a particular analysis, exactly where we think we need them."

Based on the author's explanation, multilevel models sound really promising, and I'm eager to learn more about them.

### *Model comparison and information criteria*

Information criteria were developed to compare "structurally different models based upon future predictive accuracy, using different approximations for different model types."

Popular information criterion AIC, and related metrics- such as DIC and WAIC- "explicitly build a model of the prediction task and use that model to estimate performance of each model you might wish to compare. Because the prediction is modeled, it depends upon assumptions. So information criteria do not in fact achieve the impossible, by seeing the future. They are still golems." The author also mentioned that they are known as "information criteria, because they develop their measure of model accuracy from information theory."

These criteria can help researchers with two common difficulties in model comparison:

(1)  ***Overfitting.*** Future data will not be exactly like past data, and so any model that is unaware of this fact tends to make worse predictions than it could. So if we wish to make good predictions, we cannot judge our models simply on how well they fit our data. Information criteria provide estimates of predictive accuracy, rather than merely fit. So they compare models where it matters.

(2) ***Comparison of multiple non-null models to the same data.*** Frequently, several plausible models of a phenomenon are known to us. In some empirical contexts, like social networks and evolutionary phylogenies, there are no reasonable or uniquely “null” models. This was also true of the neutral evolution example. In such cases, it’s not only a good idea to explicitly compare models. It’s also mandatory. Information criteria aren’t the only way to conduct the comparison. But they are an accessible and widely used way.

$~$


# Chapter 2: Small Worlds and Large Worlds

$~$

## The garden of forking data


"In order to make good inference about what actually happened, it helps to consider everything that could have happened. A Bayesian analysis is a garden of forking data, in which alternative sequences of events are cultivated. As we learn about what did happen, some of these alternative sequences are pruned. In the end, what remains is only what is logically consistent with our knowledge."

### Counting possibilities

<span style="color:green">***A bag of avocado and pear (total 8)***</span>

- Conjectures: (a,a,a,a,a,a,a,a); (p,a,a,a,a,a,a,a); (p,p,a,a,a,a,a,a); (p,p,p,a,a,a,a,a); (p,p,p,p,a,a,a,a);
               (p,p,p,p,p,a,a,a); (p,p,p,p,p,p,a,a); (p,p,p,p,p,p,p,a); (p,p,p,p,p,p,p,p)

<span style="color:green">***Shake the bag to give equal chance and pick 4 fruits; replace the fruit after each pick***</span>

- Data: (a,p,a,a)

<span style="color:green">***Count the possible ways to have that exact data sequence for each conjecture***</span>

- (a,a,a,a,a,a,a,a)       &rarr;    8 * 0 * 8 * 8 = 0

- (p,a,a,a,a,a,a,a)       &rarr;    7 * 1 * 7 * 7 = 343         

- (p,p,a,a,a,a,a,a)       &rarr;    6 * 2 * 6 * 6 = 432

- (p,p,p,a,a,a,a,a)       &rarr;    5 * 3 * 5 * 5 = 375

- (p,p,p,p,a,a,a,a)       &rarr;    4 * 4 * 4 * 4 = 256
               
- (p,p,p,p,p,a,a,a)       &rarr;    3 * 5 * 3 * 3 = 135

- (p,p,p,p,p,p,a,a)       &rarr;    2 * 6 * 2 * 2 = 48

- (p,p,p,p,p,p,p,a)       &rarr;    1 * 7 * 1 * 1 = 7

- (p,p,p,p,p,p,p,p)       &rarr;    0 * 8 * 0 * 0 = 0


"So what good are these counts? By comparing these counts, we have part of a solution for a way to rate the relative plausibility of each conjectured bag composition. But it’s only a part of a solution, because in order to compare these counts we first have to decide how many ways each conjecture could itself be realized. We might argue that when we have no reason to assume otherwise, we can just consider each conjecture equally plausible and compare the counts directly. But often we do have reason to assume otherwise."


### Using prior information 

Two possible choices to incorporate prior information: add a new layer to the garden or multiply prior count by the new count. "It turns out that these two methods are mathematically identical, as long as the new observation is logically independent of the previous observations."

"First we count the numbers of ways each conjecture could produce the new observation. Then we multiply each of these new counts by the previous numbers of ways for each conjecture."

We select one more fruit from the bag. Now the data sequence is "a,p,a,a,p"

- (a,a,a,a,a,a,a,a)       &rarr;       0 * 0 = 0

- (p,a,a,a,a,a,a,a)       &rarr;     343 * 1 = 343            

- (p,p,a,a,a,a,a,a)       &rarr;     432 * 2 = 864

- (p,p,p,a,a,a,a,a)       &rarr;     375 * 3 = 1125

- (p,p,p,p,a,a,a,a)       &rarr;     256 * 4 = 1024
               
- (p,p,p,p,p,a,a,a)       &rarr;     135 * 5 = 675

- (p,p,p,p,p,p,a,a)       &rarr;      48 * 6 = 288

- (p,p,p,p,p,p,p,a)       &rarr;       7 * 7 = 49

- (p,p,p,p,p,p,p,p)       &rarr;       0 * 8 = 0

"This updating approach amounts to nothing more than asserting that (1) when we have previous information suggesting there are W~prior~ ways for a conjecture to produce a previous observation D~prior~ and (2) we acquire new observations D~new~ that the same conjecture can produce in W~new~ ways, then (3) the number of ways the conjecture can account for both D~prior~ as well as D~new~ is just the product W~prior~ × W~new~."

$~$

<span style="color:brown"><span style="text-decoration:underline">**Incorporating new data types different from prior data**</span>

*New information: There are fewer pears than avocados. For every bag containing 5 pears and 3 avocados, there are two bags containing 4 pears and 4 avocados, three bags containing 5 avocados and 3 pears, and four bags containing 2 pears and 6 avocados. Each bag contains at least 2 pears and 2 avocados.*

- (a,a,a,a,a,a,a,a)       &rarr;       0 * 0 = 0

- (p,a,a,a,a,a,a,a)       &rarr;     343 * 0 = 0            

- (p,p,a,a,a,a,a,a)       &rarr;     864 * 4 = 3456

- (p,p,p,a,a,a,a,a)       &rarr;    1125 * 3 = 3375

- (p,p,p,p,a,a,a,a)       &rarr;    1024 * 2 = 2048
               
- (p,p,p,p,p,a,a,a)       &rarr;     675 * 1 = 675

- (p,p,p,p,p,p,a,a)       &rarr;     288 * 0 = 0

- (p,p,p,p,p,p,p,a)       &rarr;      49 * 0 = 0

- (p,p,p,p,p,p,p,p)       &rarr;       0 * 0 = 0


I made a mental note in the beginning that the bag contained 4 pears and 4 avocados. Now, using the counting method has gotten me nearer to the truth, but it makes no difference since I will get the wrong answer anyway. The author asked "Is there a threshold difference in these counts at which we can safely decide that one of the conjectures is the correct one?", when one conjecture is barely more possible than the other. I think the answer is no (correct answer is in chapter 3). 

"Which assumption should we use, when there is no previous information about the conjectures? The most common solution is to assign an equal number of ways that each conjecture could be correct, before seeing any data. This is sometimes known as the *principle of indifference*: When there is no reason to say that one conjecture is more plausible than another, weigh all of the conjectures equally. The issue of choosing a representation of “ignorance” is surprisingly complicated. The issue will arise again in later chapters. For the sort of problems we examine in this book, the principle of indifference results in inferences very comparable to mainstream non- Bayesian approaches, most of which contain implicit equal weighting of possibilities. For example a typical non-Bayesian confidence interval weighs equally all of the possible values a parameter could take, regardless of how implausible some of them are. Many non-Bayesian procedures have moved away from this, through the use of penalized likelihood and other methods."

$~$

### From counts to probability

*If p is the proportion of pear and D~new~ = {a,p,a,a}*

plausibility of p after D~new~ $∝$ ways p can produce D~new~ × prior plausibility of p

"The above just means that for any value p can take, we judge the plausibility of that value p as proportional to the number of ways it can get through the garden of forking data. This expression just summarizes the calculations you did in the tables of the previous section.

Finally, we construct probabilities by standardizing the plausibility so that the sum of the plausibilities for all possible conjectures will be one. All you need to do in order to standardize is to add up all of the products, one for each value p can take, and then divide each product by the sum of products:"

plausibility of p after D~new~ = $\frac{ways~p~can~produce~D~new~ x~ prior~plausibility~p}{sum~of~products}$


```{r table, tidy=FALSE, include=TRUE, echo=FALSE}
data2 <- read.table(stringsAsFactors = FALSE, header = TRUE, sep="/", text =
'Possible_composition/ p/ Ways_to_produce_data/ Plausibility
 (a,a,a,a,a,a,a,a)/ 0.00/ 0.00/ 0.00
 (p,a,a,a,a,a,a,a)/ 0.125/ 343/ 0.21         
 (p,p,a,a,a,a,a,a)/ 0.25/ 432/ 0.27
 (p,p,p,a,a,a,a,a)/ 0.375/ 375/ 0.23
 (p,p,p,p,a,a,a,a)/ 0.5/ 256/ 0.16
 (p,p,p,p,p,a,a,a)/ 0.625/ 135/ 0.08
 (p,p,p,p,p,p,a,a)/ 0.75/ 48/ 0.03
 (p,p,p,p,p,p,p,a)/ 0.875/ 7/ 0.004
 (p,p,p,p,p,p,p,p)/ 1/ 0/ 0'
)
knitr::kable((data2), booktabs = TRUE)
```

OR

```{r}

ways <- c(0, 343, 432, 375, 256, 135, 48, 7, 0)
ways/sum(ways)

```

"These plausibilities are also probabilities— they are non-negative (zero or positive) real numbers that sum to one. And all of the mathematical things you can do with probabilities you can also do with these values. Specifically, each piece of the calculation has a direct partner in applied probability theory. These partners have stereotyped names, so it’s worth learning them, as you’ll see them again and again.

• A conjectured proportion of pears, p, is usually called a parameter value. It’s just a way of indexing possible explanations of the data.

• The relative number of ways that a value p can produce the data is usually called a likelihood. It is derived by enumerating all the possible data sequences that could have happened and then eliminating those sequences inconsistent with the data.

• The prior plausibility of any specific p is usually called the prior probability.

• The new, updated plausibility of any specific p is usually called the posterior
probability.

*If we really have shuffled a deck of cards enough to erase any prior knowledge of the ordering- in randomization process-, then the order the cards end up in is very likely to be one of the many orderings with high information entropy.*"

$~$

## Building a model

"Designing a simple Bayesian model benefits from a design loop with three steps.

(1) Data story: Motivate the model by narrating how the data might arise.

(2) Update: Educate your model by feeding it the data.

(3) Evaluate: All statistical models require supervision, leading possibly to model revision."

$~$

### A data story

"Bayesian data analysis usually means producing a story for how the data came to be. This story may be descriptive, specifying associations that can be used to predict outcomes, given observations. Or it may be causal, a theory of how some events produce other events. Typically, any story you intend to be causal may also be descriptive. But many descriptive stories are hard to interpret causally. But all data stories are complete, in the sense that they are sufficient for specifying an algorithm for simulating new data. 

You can motivate your data story by trying to explain how each piece of data is born. This usually means describing aspects of the underlying reality as well as the sampling process.

The data story is then translated into a formal probability model. This probability model is easy to build, because the construction process can be usefully broken down into a series of component decisions."

$~$

### Bayesian updating

"A Bayesian model begins with one set of plausibilities assigned to each of these possibilities. These are the prior plausibilities. Then it updates them in light of the data, to produce the posterior plausibilities. This updating process is a kind of learning, called Bayesian updating.

Notice that every updated set of plausibilities becomes the initial plausibilities for the next observation. Every conclusion is the starting point for future inference. However, this updating process works backwards, as well as forwards. Knowing the final observation, it is possible to mathematically divide out the observation, to infer the previous plausibility curve. So the data could be presented to your model in any order, or all at once even. In most cases, you will present the data all at once, for the sake of convenience. But it’s important to realize that this merely represents abbreviation of an iterated learning process.

Bayesian estimates are valid for any sample size. This does not mean that more data isn’t helpful—it certainly is. Rather, the estimates have a clear and valid interpretation, no matter the sample size. But the price for this power is dependency upon the initial estimates, the prior. If the prior is a bad one, then the resulting inference will be misleading."

$~$

### Evaluate

"If there are important differences between the model and reality, then there is no logical guarantee of large world performance. And even if the two worlds did match, any particular sample of data could still be misleading. So it’s worth keeping in mind at least two cautious principles: (1) the model’s certainty is no guarantee that the model is a good one and (2) it is important to supervise and critique your model’s work.

Note that the goal is not to test the truth value of the model’s assumptions. All manner of small world assumptions about error distributions and the like can be violated in the large world, but a model may still produce a perfectly useful estimate. Instead, the objective is to check the model’s adequacy for some purpose. This usually means asking and answering additional questions, beyond those that originally constructed the model."

$~$

## Components of the model

"The usual way we build a statistical model involves choosing distributions and devices for each that represent the relative numbers of ways things can happen. These distributions and devices are 

(1) a likelihood function- the number of ways each conjecture could produce an observation

(2) one or more parameters- the accumulated number of ways each conjecture could produce the entire data

(3) a prior- the initial plausibility of each conjectured cause of the data"

$~$

### Likelihood

"The likelihood is a mathematical formula that specifies the plausibility of the data. What this means is that the likelihood maps each conjecture onto the relative number of ways the data could occur, given that possibility."

<span style="color:brown"><span style="text-decoration:underline">**Binomial distribution**</span>

- each event is independent of other events

- the probability of "an observation" is the same on every event

- the common "coin tossing" distribution where there is only two possible answers (e.g., land or water in globe example; failure or success, etc.)

In the given globe example, there are two possible outcomes: either land or water. Out of 9 tosses (n), 6 tosses land on water. The probability of getting water (p) is the same (0.5%) on every toss. Then, we can use binomial distribution formula in R to compute the "likelihood of data w (water)- 6 W's in 9 tosses- under any value of p with":

```{r}

dbinom( 6, size = 9, prob = 0.5)

```


If probability for getting water is 0.2,

``` {r}

dbinom(6, size = 9, prob = 0.2)

```

*"The job of the likelihood is to tell us the relative number of ways to see the data w, given values for p and n."*

$~$

### Parameters

"It is typical to conceive of data and parameters as completely different kinds of entities. Data are measured and known; parameters are unknown and must be estimated from data. Usefully, in the Bayesian framework the distinction between a datum and a parameter is fuzzy. A datum can be recast as a very narrow probability density for a parameter, and a parameter as a datum with uncertainty. This continuity between certainty (data) and uncertainty (parameters) can be exploited (Ch 14) to incorporate measurement error and missing data into your modeling.

In globe tossing example, n and w are data, and p is unknown parameter. Your Bayesian machine can tell you what the data say about any parameter, once you tell it the likelihood and which bits have been observed. In some cases, the golem will just tell you that not much can be learned—Bayesian data analysis isn’t magic, after all. But it is usually an advance to learn that our data don’t discriminate among the possibilities."

$~$

### Prior

"A Bayesian machine must have an initial plausibility assignment for each possible value of the parameter. The prior is this initial set of plausibilities. Priors are useful for constraining parameters to reasonable ranges, as well as for expressing any knowledge we have about the parameter, before any data are observed.

Within Bayesian data analysis in the natural and social sciences, the prior is considered to be just part of the model. As such it should be chosen, evaluated, and revised just like all of the other components of the model. In practice, the subjectivist and the non-subjectivist will often analyze data in nearly the same way.

If you don’t have a strong argument for any particular prior, then try different ones. Because the prior is an assumption, it should be interrogated like other assumptions: by altering it and checking how sensitive inference is to the assumption."

$~$

### Posterior

"Once you have chosen a likelihood, which parameters are to be estimated, and a prior for each parameter, a Bayesian model treats the estimates as a purely logical consequence of those assumptions. For every unique combination of data, likelihood, parameters, and prior, there is a unique set of estimates. The resulting estimates—the relative plausibility of different parameter values, conditional on the data—are known as the posterior distribution. The posterior distribution takes the form of the probability of the parameters, conditional on the data: `Pr(p|n, w).`

<span style="color:brown"><span style="text-decoration:underline">**Bayes Theorem**</span>

The mathematical procedure that defines the logic of the posterior distribution is Bayes’ theorem. The joint probability of the data w and any particular value of p is: `Pr(w, p) = Pr(w|p) Pr(p)`- the probability of w and p is the product of likelihood Pr(w|p) and the prior probability Pr(p)."

Example- the probability of mango both sweet and yellow is equal to the probability that mango is sweet given that it is yellow, multiplies by the probability that it is yellow: `Pr(s, y) = Pr(s|y) Pr(y)`

OR the probability of mango both sweet and yellow is equal to the probability that mango is yellow given that it is sweet, multiplies by the probability that it is sweet: `Pr(s, y) = Pr(y|s) Pr(s)`

Now since both right-hand sides are equal to the same thing, we can set them equal to one another and solve for the posterior probability: Pr(s|y) = $\frac{Pr(y|s)~Pr(s)}{Pr(y)}$

<span style="color:brown">***Bayes' theorem***</span>: the probability of any particular value of p, considering the data, is equal to the product of the likelihood and prior, divided by this thing Pr(w), which I’ll call the *average likelihood*. In word form:

$$\text{Posterior =}\frac{\text{Likelihood x Prior}}{\text{Average Likelihood}}$$

<span style="color:brown">***Average Likelihood:***</span>

- commonly called "evidence" or "probability of the data"

- Likelihood of the data averaged over the prior

- To standardize the posterior, to ensure it sums (integrates) to one 

$~$

## Making the model go 

"Bayesian model is a machine, a figurative golem. It has built-in definitions for the likelihood, the parameters, and the prior. And then at its heart lies a motor that processes data, producing a posterior distribution. The action of this motor can be thought of as conditioning the prior on the data.

Three different conditioning engines, numerical techniques for computing posterior distributions, that can accommodate whichever prior is most useful for inference:

(1) Grid approximation

(2) Quadratic approximation

(3) Markov chain Monte Carlo (MCMC)"

$~$

### Grid approximation

While most parameters are continuous, capable of taking on an infinite number of values, it turns out that we can achieve an excellent approximation of the continuous posterior distribution by considering only a finite grid of parameter values. At any particular value of a parameter, p′, it’s a simple matter to compute the posterior probability: just multiply the prior probability of p′ by the likelihood at p′. Repeating this procedure for each value in the grid generates an approximate picture of the exact posterior distribution. This procedure is called grid approximation. 

But in most of your real modeling, grid approximation isn’t practical. The reason is that it scales very poorly, as the number of parameters increases. 

<span style="color:brown"><span style="text-decoration:underline">**Building a grid approximation**</span>

1. Define the grid- decide how many points to use in estimating the posterior, and then make a list of the parameter values on the grid. 

2. Compute the value of the prior at each parameter value on the grid.

3. Compute the likelihood at each parameter value.

4. Compute the unstandardized posterior at each parameter value, by multiplying the prior by the likelihood.

5. Finally, standardize the posterior, by dividing each value by the sum of all values.

$~$

<span style="color:gray">***Building a grid approximation for globe tossing example***</span>

``` {r globe tossing example}
# define grid
p_grid <- seq( from=0 , to=1 , length.out=20 )

# define prior
prior <- rep( 1 , 20 )

# compute likelihood at each value in grid
likelihood <- dbinom( 6 , size=9 , prob=p_grid )

# compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# Display the posterior distribution 

plot( p_grid , posterior , type="b" ,
    xlab="probability of water" , ylab="posterior probability" )
mtext( "20 points" )

```

$~$

<span style="color:gray">***Try with a 5-point grid***</span>

``` {r 5-point grid}

# define grid

p_grid <- seq(from = 0, to = 1, length.out = 5)

# define prior

prior <- rep(1, 5) # 1 form a uniform prior distribution, assigning equal probability to each value in the grid. Using a decimal here will mean the same but may assume less degree of uncertainty?

# compute likelihood at each value in the grid

likelihood <- dbinom(6, size = 9, prob = p_grid)

# compute product of likelihood and prior

unstd.posterior <- likelihood * prior

# standardize the posterior, so it sums to 1

std.posterior <- unstd.posterior / sum(unstd.posterior)

# plot the posterior distribution

plot(p_grid, std.posterior, type = "b",
     xlab = "probability of water", ylab = "posterior probability")
mtext("5 points")

```


$~$

<span style="color:gray">***Try with a 100-point grid***</span>

```{r 100-point grid}

# define a grid

p_grid <- seq(from = 0, to = 1, length.out = 100)

# define a prior

prior <- rep(1, 100)

# compute likelihood at each value in grid

likelihood <- dbinom(6, size = 9, prob = p_grid)

# compute the unstandardized posterior

unstd.posterior <- likelihood * prior

# standardize the posterior 

posterior <- unstd.posterior / sum(unstd.posterior)

# plot the posterior

plot(p_grid, posterior, type = "b",
     xlab = "probability of water", ylab = "posterior distribution")
mtext("100 points")

```

*In this simple example, you can go crazy and use 100,000 points, but there won’t be much change in inference after the first 100.*

$~$

<span style="color:gray">***Try with a step prior***</span>

```{r step prior}

# define a grid

p_grid <- seq(from = 0, to = 1, length.out = 40)

# define a prior

prior <- ifelse(p_grid < 0.5, 0, 1) #ifelse(test, yes, no)

# calculate the likelihood

likelihood <- dbinom(6, size = 9, prob = p_grid)

# calculate unstandardized posterior

unstd.posterior <- likelihood * prior

# standardize the posterior

posterior <- unstd.posterior / sum(unstd.posterior)

# plot the posterior

plot(p_grid, posterior, type = "o",
     xlab = "probability of water", ylab = "posterior distribution")
mtext("40 points")

```

$~$

<span style="color:gray">***Try with a peaked prior***</span>

```{r peaked prior}

# define a grid

p_grid <- seq(from = 0, to = 1, length.out = 40)

# define a prior

prior <- exp(-5*abs(p_grid - 0.5))

# calculate the likelihood

likelihood <- dbinom(6, size = 9, prob = p_grid)

# calculate unstandardised posterior

unstd.posterior <- likelihood * prior

# standardize the posterior

posterior <- unstd.posterior / sum(unstd.posterior)

# plot the posterior distribution

plot(p_grid, posterior, type = "b",
     xlab = "probability of water", ylab = "posterior distribution")
mtext("40 points")

```

$~$

### Quadratic approximation

Under quite general conditions, the region near the peak of the posterior distribution will be nearly Gaussian—or “normal”—in shape. This means the posterior distribution can be usefully approximated by a Gaussian distribution. A Gaussian distribution is convenient, because it can be completely described by only two numbers: the location of its center (mean) and its spread (variance).

A Gaussian approximation is called “quadratic approximation” because the logarithm of a Gaussian distribution forms a parabola. And a parabola is a quadratic function. So this approximation essentially represents any log-posterior with a parabola.

The procedure to carry out quadratic approximation in R contains two steps:

(1) Find the posterior mode. This is usually accomplished by some optimization algorithm, a procedure that virtually “climbs” the posterior distribution, as if it were a mountain. The golem doesn’t know where the peak is, but it does know the slope under its feet. There are many well-developed optimization procedures, most of them more clever than simple hill climbing. But all of them try to find peaks.

(2) Once you find the peak of the posterior, you must estimate the curvature near the peak. This curvature is sufficient to compute a quadratic approximation of the entire posterior distribution. In some cases, these calculations can be done analytically, but usually your computer uses some numerical technique instead.


<span style="color:gray">***Compute the quadratic approximation to the globe tossing data***</span>

```{r quadratic approximation}

library(rethinking)

globe.qa <- map(           #  map is maximum a posteriori- the mode of the posterior distribution
    alist(
        w ~ dbinom(9,p) ,  # binomial likelihood
        p ~ dunif(0,1)     # uniform prior
), data=list(w=6) )

# display summary of quadratic approximation
precis( globe.qa )

```

*To use map, you provide a formula, a list of data, and a list of start values for the parameters. The formula defines the likelihood and prior.*

The function precis presents a brief summary of the quadratic approximation. In this case, it shows a MAP value of p = 0.67, which it calls the “Mean.” The curvature is labeled “Std- Dev” This stands for standard deviation. This value is the standard deviation of the posterior distribution, while the mean value is its peak. Finally, the last two values in the precis output show the 89% percentile interval. You can read this kind of approximation like: Assuming the posterior is Gaussian, it is maximized at 0.67, and its standard deviation is 0.16.

Since we already know the posterior, let’s compare to see how good the approximation is. I’ll use the analytical approach here, which uses dbeta.

```{r dbeta}

# analytical calculation
w <- 6
n <- 9
curve( dbeta( x , w+1 , n-w+1 ) , from=0 , to=1 ) 
# Use Laplace's rule of succession- add one to both w and n-w to avoid assigning zero probability, 
# ensure at least there's one w and one n-w, even if none have been observed yet

# quadratic approximation
curve( dnorm( x , 0.67 , 0.16 ) , lty=2 , add=TRUE )

```


<span style="color:gray">***Double the sample size***</span>

```{r quadratic approximation1}

library(rethinking)

globe.qa1 <- map(           #  map is maximum a posteriori- the mode of the posterior distribution
    alist(
        w ~ dbinom(18,p) ,  # binomial likelihood
        p ~ dunif(0,1)     # uniform prior
), data=list(w=12) )

# display summary of quadratic approximation
precis( globe.qa1 )

```

```{r increase tosses1}

# analytical calculation
w <- 12
n <- 18
curve( dbeta( x , w+1 , n-w+1 ) , from=0 , to=1 )

# quadratic approximation
curve( dnorm( x , 0.67 , 0.11 ) , lty=2 , add=TRUE )

```

<span style="color:gray">***Quadruple the sample size***</span>

```{r quadratic approximation2}

library(rethinking)

globe.qa2 <- map(           #  map is maximum a posteriori- the mode of the posterior distribution
    alist(
        w ~ dbinom(36,p) ,  # binomial likelihood
        p ~ dunif(0,1)     # uniform prior
), data=list(w=24) )

# display summary of quadratic approximation
precis( globe.qa2 )

```

```{r increase tosses2}

# analytical calculation
w <- 24
n <- 36
curve( dbeta( x , w+1 , n-w+1 ) , from=0 , to=1 )

# quadratic approximation
curve( dnorm( x , 0.67 , 0.078 ) , lty=2 , add=TRUE )

```


This phenomenon, where the quadratic approximation improves with the amount of data, is very common. It’s one of the reasons that so many classical statistical procedures are nervous about small samples: Those procedures use quadratic (or other) approximations that are only known to be safe with infinite data. Often, these approximations are useful with less than infinite data, obviously. But the rate of improvement as sample size increases varies greatly depending upon the details. In some model types, the quadratic approximation can remain terrible even with thousands of samples.

***Maximum likelihood estimation:*** The quadratic approximation, either with a uniform prior or with a lot of data, is usually equivalent to a maximum likelihood estimate (MLE) and its standard error. The MLE is a very common non-Bayesian parameter estimate. This equivalence between a Bayesian approximation and a common non-Bayesian estimator is both a blessing and a curse. It is a blessing, because it allows us to re-interpret a wide range of published non-Bayesian model fits in Bayesian terms. It is a curse, because maximum likelihood estimates have some curious drawbacks.

$~$

### Markov chain Monte Carlo

The conceptual challenge with MCMC lies in its highly non-obvious strategy. Instead of attempting to compute or approximate the posterior distribution directly, MCMC techniques merely draw samples from the posterior. You end up with a collection of parameter values, and the frequencies of these values correspond to the posterior plausibilities. You can then build a picture of the posterior from the histogram of these samples.

We nearly always work directly with these samples, rather than first constructing some mathematical estimate from them. And the samples are in many ways more convenient than having the posterior, because they are easier to think with.

$~$

## Practice

### Easy

2E1. (2), (4)

2E2. (3)

2E3. (1), (4)

2E4. In the context of the globe tossing example, "the probability of water is 0.7" means we estimates there is 70% chance of landing on water in the next toss, based on the observed data (the previous tosses). We may as well say 70% of the earth is water. This is a probability, but the "probability does not exist-" our best guess is 70% of earth is water based on previously observed data, but it may or may not reflect reality. If the next toss fell on land, the probability of water would be less than 70% for the subsequent toss. We can get a consistent probability of water after enough tosses, but we cannot say for sure this is the actual distribution of water on earth. Therefore, it remains as a probability. Once we have a concrete measurement of actual percentage of earth covered by water, it becomes an objective reality- a factual statement. At that point, the need for probability ceases.

$~$

### Medium

2M1. 

<span style= "color: blue">*(1) W, W, W*</span>

```{r 2M11}

# define a grid

p_grid <- seq(from = 0, to = 1, length.out = 100)  
# choose 100 coz chapter says not much difference after 100

# define a uniform prior

prior <- rep(1, 100) 

# calculate likelihood

likelihood <- dbinom(3, size = 3, prob = p_grid)

# calculate unstandardized posterior

unstd.posterior <- likelihood * prior

# standardize the posterior

posterior <- unstd.posterior / sum(unstd.posterior)

# plot the posterior distribution

plot(p_grid, posterior, type = "l",
    xlab = "probability of water", ylab = "posterior distribution")
  mtext("100 points")
  
```

$~$

<span style= "color: blue">*(2) W, W, W, L*</span>

```{r 2M12}

# define a grid

p_grid <- seq(from = 0, to = 1, length.out = 100)

# define a uniform prior

prior <- rep(1, 100) 

# calculate likelihood

likelihood <- dbinom(3, size = 4, prob = p_grid)

# calculate unstandardized posterior

unstd.posterior <- likelihood * prior

# standardize the posterior

posterior <- unstd.posterior / sum(unstd.posterior)

# plot the posterior distribution

plot(p_grid, posterior, type = "l",
    xlab = "probability of water", ylab = "posterior distribution")
  mtext("100 points")
  
```

$~$

<span style= "color: blue">*(3) L, W, W, L, W, W, W*</span>

```{r 2M13}

# define a grid

p_grid <- seq(from = 0, to = 1, length.out = 100)

# define a uniform prior

prior <- rep(1, 100) 

# calculate likelihood

likelihood <- dbinom(5, size = 7, prob = p_grid)

# calculate unstandardized posterior

unstd.posterior <- likelihood * prior

# standardize the posterior

posterior <- unstd.posterior / sum(unstd.posterior)

# plot the posterior distribution

plot(p_grid, posterior, type = "l",
    xlab = "probability of water", ylab = "posterior distribution")
  mtext("100 points")
  
```

$~$

2M2. Assume a prior for p that is equal to zero when p < 0.5 and is a positive constant when p ≥ 0.5. 

<span style= "color: blue">*(1) W, W, W*</span>

```{r 2M21}

# define a grid

p_grid <- seq(from = 0, to = 1, length.out = 100)  

# define a  prior

prior <- ifelse(p_grid < 0.5, 0, 1) 

# calculate likelihood

likelihood <- dbinom(3, size = 3, prob = p_grid)

# calculate unstandardized posterior

unstd.posterior <- likelihood * prior

# standardize the posterior

posterior <- unstd.posterior / sum(unstd.posterior)

# plot the posterior distribution

plot(p_grid, posterior, type = "l",
    xlab = "probability of water", ylab = "posterior distribution")
  mtext("100 points")
  
```

$~$

<span style= "color: blue">*(2) W, W, W, L*</span>

```{r 2M22}

# define a grid

p_grid <- seq(from = 0, to = 1, length.out = 100)

# define a prior

prior <- ifelse(p_grid < 0.5, 0, 1)  

# calculate likelihood

likelihood <- dbinom(3, size = 4, prob = p_grid)

# calculate unstandardized posterior

unstd.posterior <- likelihood * prior

# standardize the posterior

posterior <- unstd.posterior / sum(unstd.posterior)

# plot the posterior distribution

plot(p_grid, posterior, type = "l",
    xlab = "probability of water", ylab = "posterior distribution")
  mtext("100 points")
  
```
    
$~$

<span style= "color: blue">*(3) L, W, W, L, W, W, W*</span>

```{r 2M23}

# define a grid

p_grid <- seq(from = 0, to = 1, length.out = 100)

# define a prior

prior <- ifelse(p_grid < 0.5, 0, 1) 

# calculate likelihood

likelihood <- dbinom(5, size = 7, prob = p_grid)

# calculate unstandardized posterior

unstd.posterior <- likelihood * prior

# standardize the posterior

posterior <- unstd.posterior / sum(unstd.posterior)

# plot the posterior distribution

plot(p_grid, posterior, type = "l",
    xlab = "probability of water", ylab = "posterior distribution")
  mtext("100 points")
  
```

$~$

2M3. Suppose there are two globes, one for Earth and one for Mars. The Earth globe is 70% covered in water. The Mars globe is 100% land. Further suppose that one of these globes—you don’t know which—was tossed in the air and produced a “land” observation. Assume that each globe was equally likely to be tossed. Show that the posterior probability that the globe was the Earth, conditional on seeing “land” (Pr(Earth|land)), is 0.23.

```

# Probability land on earth, given that 70% of earth is water
Pr(land|earth) = 0.3

# Probability of land on mars
Pr(land|mars) = 1 

# Probability that earth is tossed
Pr(earth) = 0.5

# Probability that mars is tossed
Pr(mars) = 0.5

# Probability that it is both land and earth
Pr(land, earth) = Pr(land|earth) * Pr(earth)
                = 0.3 * 0.5
                = 0.15

# Probability that it is both land and mars
Pr(land, mars) = Pr(land|mars) * Pr(mars)
               = 1 * 0.5
               = 0.5

# Total probability of land in all possible scenarios (both on earth and mars)
Pr(land) = Pr(land, earth) + Pr(land, mars)
         = 0.15 + 0.5
         = 0.65

# Find the probability of earth, conditional on seeing land
Pr(earth|land) = (Pr(land|earth) * Pr(earth))/ Pr(land)
               = (0.3 * 0.5)/ 0.65
               = 0.2307692

```

$~$

2M4. Suppose you have a deck with only three cards. Each card has two sides, and each side is either black or white. One card has two black sides. The second card has one black and one white side. The third card has two white sides. Now suppose all three cards are placed in a bag and shuffled. Someone reaches into the bag and pulls out a card and places it flat on a table. A black side is shown facing up, but you don’t know the color of the side facing down. Show that the probability that the other side is also black is 2/3. Use the counting method (Section 2 of the chapter) to approach this problem. This means counting up the ways that each card could produce the observed data (a black side facing up on the table).

```
# First card has two black sides
C1 <- b,b

# Second card has one black and one white side
C2 <- b,w

# Third card has two white sides
C3 <- w,w

# Possible conjectures 
conjectures <- (b,b), (b,w), (w,w)

# One card is drawn with black side facing up
data <- b

# Count the possible ways to have the observe data for each conjecture (data: b)
(b,b) -> 2  
(b,w) -> 1
(w,w) -> 0
total ways to see a black side up -> 3

# Count the possible ways that the other side is also black (data: b,b)
(b,b) -> 2 x 1 -> 2
(b,w) -> 1 x 0 -> 0
(w,w) -> 0 x 0 -> 0
total ways to see another side also black -> 2

# Possible ways to get both side black
(b,b) -> total ways to see another side also black / total ways to see a black side up
      -> 2/3
      

```
<span style="color:blue">***If we calculate probability as shown in chapter 2***</span>

```{r 2M4}

ways <- c(2, 1, 0)
ways/ sum(ways)

```

$~$

2M5. Now suppose there are four cards: B/B, B/W, W/W, and another B/B. Again suppose a card is drawn from the bag and a black side appears face up. Again calculate the probability that the other side is black.

```
# Possible conjectures
conjectures: (B/B), (B/W), (W/W), (B/B)

# First observation
data: B

# Possible ways to get the observed data for each conjecture
(B/B) -> 2
(B/W) -> 1
(W/W) -> 0
(B/B) -> 2
Total ways to get initial black side -> 5

# Probability to see black on second observation (data: B/B)
(B/B) -> 2 x 1 = 2
(B/W) -> 1 x 0 = 0
(W/W) -> 0 x 0 = 0
(B/B) -> 2 x 1 = 2
Total ways to get another black side -> 4

Hence, the probability of getting a card with both sides black by counting method is 4/5. 

```
$~$

2M6. Imagine that black ink is heavy, and so cards with black sides are heavier than cards with white sides. As a result, it’s less likely that a card with black sides is pulled from the bag. So again assume there are three cards: B/B, B/W, and W/W. After experimenting a number of times, you conclude that for every way to pull the B/B card from the bag, there are 2 ways to pull the B/W card and 3 ways to pull the W/W card. Again suppose that a card is pulled and a black side appears face up. Show that the probability the other side is black is now 0.5. Use the counting method, as before.

``` 
# Possible conjectures
conjectures : (B/B), (B/W), (W/W)

# New information
Pr(B/B) < Pr(B/W) < Pr(W/W)
1 (B/B) = 2 (B/W) + 3 (W/W)

# Likelihood of drawing each card
(B/B) -> 1
(B/W) -> 2
(W/W) -> 3

# Count the possible ways to get data (B)
(B/B) -> 2 x 1 = 2
(B/W) -> 1 x 2 = 2
(W/W) -> 0 x 3 = 0
Total counts of way to get initial B = 4

# Count the possible ways to get data (B/B)
(B/B) -> 2 x 1 = 2
(B/W) -> 2 x 0 = 0
(W/W) -> 0 x 0 = 0
Total counts of way to get B/B = 2

Probability the other side is also black = 2/4 = 0.5

```
$~$

2M7. Assume again the original card problem, with a single card showing a black side face up.Before looking at the other side, we draw another card from the bag and lay it face up on the table. The face that is shown on the new card is white. Show that the probability that the first card, the one showing a black side, has black on its other side is now 0.75. Use the counting method, if you can. Hint: Treat this like the sequence of globe tosses, counting all the ways to see each observation, for each possible first card.

```
# Possible conjectures
conjectures: (B/B), (B/W), (W/W)

# Count the possible ways to get data (B)
(B/B) -> 2 
(B/W) -> 1 
(W/W) -> 0 

# Count the possible ways to get data (W)
(B/B) -> 0
(B/W) -> 1 
(W/W) -> 2 

# Count the possible ways to get data (B/x, W/x)
(B/B) , (B/W) -> 2 x 1 = 2
(B/B) , (W/W) -> 2 x 2 = 4
(B/W) , (B/B) -> 1 x 0 = 0
(B/W) , (W/W) -> 1 x 2 = 2
Total counts of ways to get data (B/x, W/x) = 8

# Count the possible ways to get data (B/B, W/x)
(B/B) , (B/W) -> 2 x 1 = 2
(B/B) , (W/W) -> 4 x 1 = 4
(B/W) , (B/B) -> 0 x 0 = 0
(B/W) , (W/W) -> 2 x 0 = 0
Total counts of ways to get data (B/B, W/x) = 6

Probability that the first card has both black sides = 6/8 = 0.75

```
$~$

### Hard

2H1. Suppose there are two species of panda bear. Both are equally common in the wild and live in the same places. They look exactly alike and eat the same food, and there is yet no genetic assay capable of telling them apart. They differ however in their family sizes. Species A gives birth to twins 10% of the time, otherwise birthing a single infant. Species B births twins 20% of the time, otherwise birthing singleton infants. Assume these numbers are known with certainty, from many years of field research.
Now suppose you are managing a captive panda breeding program. You have a new female panda of unknown species, and she has just given birth to twins. What is the probability that her next birth will also be twins?

```
# Probability of species A gives birth to twins
Pr(twin|SpA) = 0.1

# Probability of species B gives birth to twins
Pr(twin|SpB) = 0.2

# Probability that the new female panda belongs to species A or species B
Pr(SpA) = 0.5
Pr(SpB) = 0.5

# Total probability of having twins (average likelihood)
Pr(twins) = (Pr(twins|SpA) * Pr(SpA)) + (Pr(twins|SpB) * Pr(SpB))
          = (0.1 * 0.5) + (0.2 * 0.5)
          = 0.15
          
# Total probability of having two set of twins (average likelihood)
Pr(2twins) = (Pr(2twins|SpA) * Pr(SpA)) + (Pr(2twins|SpB) * Pr(SpB))
          = (0.01 * 0.5) + (0.04 * 0.5)
          = 0.025

# Probability of having another set of twins given that the first set is twin, regardless of species
Pr (second_twins|first_twins) = Pr(first_twins, second_twins)/ Pr(twins)
                              = Pr(2twins)/ Pr(twins)
                              = 0.025/ 0.15 
                              = 0.1666667

```
$~$

2H2. Recall all the facts from the problem above. Now compute the probability that the panda we have is from species A, assuming we have observed only the first birth and that it was twins.

``` 
# Probability of panda from species A giving birth to twins
Pr(twins|SpA) = 0.1
Pr(twin|SpB)  = 0.2

# Probability that the new female panda belongs to species A or species B
Pr(SpA) = 0.5
Pr(SpB) = 0.5

# Total probability of having twins (average likelihood)
Pr(twins) = (Pr(twins|SpA) * Pr(SpA)) + (Pr(twins|SpB) * Pr(SpB))
          = (0.1 * 0.5) + (0.2 * 0.5)
          = 0.15

# Probability that panda is from species A after observing first birth of twins
Pr(SpA|twins) = (Pr(twins|SpA) * Pr(SpA))/ Pr(twins)
              = (0.1 * 0.5)/ 0.15
              = 0.3333333

```
$~$

2H3. Continuing on from the previous problem, suppose the same panda mother has a second birth and that it is not twins, but a singleton infant. Compute the posterior probability that this panda is species A.

```
# Probability that the new female panda belongs to species A or species B
Pr(SpA) = 0.5
Pr(SpB) = 0.5

# Probability of panda from species A and B giving birth to twins
Pr(twins|SpA) = 0.1
Pr(twin|SpB)  = 0.2

# Probability of panda from species A and B giving birth to singleton
Pr(singleton|SpA) = 0.9
Pr(singleton|SpB) = 0.8

# Probability of panda from species A and B having a twin and a singleton 
Pr(singleton, twins|SpA) = 0.1 * 0.9 = 0.09
Pr(singleton, twins|SpB) = 0.2 * 0.8 = 0.16

# Total probability of having a twin and a singleton (average likelihood)
Pr(singleton, twins) = (Pr(singleton, twins|SpA) * Pr(SpA)) + (Pr(singleton, twins|SpB) * Pr(SpB))
                     = (0.09 * 0.5) + (0.16 * 0.5)
                     = 0.125

# Probability that the new panda is from species A given the birth of a singleton and a twins
Pr(SpA|sigleton, twins) = (Pr(singleton, twins|SpA) * Pr(SpA))/ Pr(singleton, twins)
                        = (0.09 * 0.5)/ 0.125
                        = 0.36

OR

# Probability of the new panda is from species A or B after the first birth of twins
Pr(SpA) = 0.33
Pr(SpB) = 0.67

# Probability of panda from species A and B giving birth to singleton
Pr(singleton|SpA) = 0.9
Pr(singleton|SpB) = 0.8

# Total probability of having singleton after the first birth of twins (average likelihood)
Pr(singleton) = (Pr(singleton|SpA) * Pr(SpA)) + (Pr(singleton|SpB) * Pr(SpB))
              = (0.9 * 0.33) + (0.8 * 0.67)
              = 0.833

# Probability that the new panda is from species A given the second birth of a singleton
Pr(SpA|singleton) = (Pr(singleton|SpA) * Pr(SpA))/ Pr(singleton)
                  = (0.9 * 0.33)/ 0.833
                  = 0.3565426

```
$~$

2H4. A common boast of Bayesian statisticians is that Bayesian inference makes it easy to use all of the data, even if the data are of different types.
So suppose now that a veterinarian comes along who has a new genetic test that she claims can identify the species of our mother panda. But the test, like all tests, is imperfect. This is the information you have about the test:
• The probability it correctly identifies a species A panda is 0.8. 
• The probability it correctly identifies a species B panda is 0.65.
The vet administers the test to your panda and tells you that the test is positive for species A. First ignore your previous information from the births and compute the posterior probability that your panda is species A. Then redo your calculation, now using the birth data as well.

``` 

# Without using birth data
# Initital probability of speacies A and B
Pr(SpA) = 0.5
Pr(SpB) = 0.5

# Probability that the test correctly identifies a species A or B panda
Pr(correct_A|SpA) = 0.8
Pr(correct_B|SpB) = 0.65

# Probability that the test wrongly identifies a species A or B panda
Pr(wrong_A|SpA) = 0.2
Pr(wrong_B|SpB) = 0.35

# Total probability that the test correctly identifies species A (average likelihood)
Pr(correct_A) = (Pr(correct_A|SpA) * Pr(SpA)) + (Pr(wrong_B|SpB) * Pr(SpB))
              = (0.8 * 0.5) + (0.35 * 0.5)
              = 0.575
              
# Probability that the panda is from species A, given the correct test result
Pr(SpA|correct_A) = (Pr(correct_A|SpA) * Pr(SpA))/ Pr(correct_A)
                  = (0.8 * 0.5)/ 0.575
                  = 0.6956522

# Using birth data
# Probability that the test correctly identifies a species A or B panda
Pr(correct_A|SpA) = 0.8
Pr(correct_B|SpB) = 0.65

# Probability that the test wrongly identifies a species A or B panda
Pr(wrong_A|SpA) = 0.2
Pr(wrong_B|SpB) = 0.35

# Prior probability of species A and B
Pr(SpA) = 0.36
Pr(SpB) = 0.64

# Total probability that the test correctly identifies species A (average likelihood)
Pr(correct_A) = (Pr(correct_A|SpA) * Pr(SpA)) + (Pr(wrong_B|SpB) * Pr(SpB))
              = (0.8 * 0.36) + (0.35 * 0.64)
              = 0.512

# Probability that the panda is from species A, given the test result
Pr(SpA|correct_A) = (Pr(correct_A|SpA) * Pr(SpA))/ Pr(correct_A)
                  = (0.8 * 0.36)/ 0.512
                  = 0.5625

```
$~$

# Chapter 3: Sampling the Imaginary

$~$

## Sampling from a grid-approximate posterior

<span style="color:brown"><span style="text-decoration:underline">**Compute the posterior for the globe tossing model, using grid approximation**</span>

```{r sampling_from_grid}

p_grid <- seq(from = 0, to = 1, length.out = 1000)

prior <- rep(1, 1000)

likelihood <- dbinom(6, size = 9, prob = p_grid)

posterior <- likelihood * prior

posterior <- posterior/ sum(posterior)

plot(posterior)

```

<span style="color:brown"><span style="text-decoration:underline">**Draw the samples**</span>

Now we wish to draw 10,000 samples from this posterior. Imagine the posterior is a bucket full of parameter values, numbers such as 0.1, 0.7, 0.5, 1, etc. Within the bucket, each value exists in proportion to its posterior probability, such that values near the peak are much more common than those in the tails. We’re going to scoop out 10,000 values from the bucket. Provided the bucket is well mixed, the resulting samples will have the same proportions as the exact posterior density. Therefore the individual values of p will appear in our samples in proportion to the posterior plausibility of each value.

```{r create_sample}

samples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)
plot(samples, xlab="sample number", ylab="proportion of water(p)")
# The workhorse here is sample, which randomly pulls values from a vector. 
# The vector in this case is p_grid, the grid of parameter values. 
# The probability of each value is given by posterior, which you computed just above.

dens(samples, xlab= "proportion of water(p)", ylab= "Density") 
# show density estimate computed from the samples
# You can see that the estimated density is very similar to ideal posterior you computed 
# via grid approximation.

# Draw more samples
samples <- sample(p_grid, prob = posterior, size = 1e6, replace = TRUE)
plot(samples, xlab="sample number", ylab="proportion of water(p)")
dens(samples, xlab= "proportion of water(p)", ylab= "Density") 
# If you draw even more samples, maybe 1e5 or 1e6, the density estimate will get more 
# and more similar to the ideal.
# All you’ve done so far is crudely replicate the posterior density you had already computed.

```

$~$

## Sampling to summarize

It is necessary to summarize and interpret the posterior distribution. Exactly how it is summarized depends upon your purpose. But common questions include:

• How much posterior probability lies below some parameter value?

• How much posterior probability lies between two parameter values?

• Which parameter value marks the lower 5% of the posterior probability?

• Which range of parameter values contains 90% of the posterior probability? 

• Which parameter value has highest posterior probability?

These simple questions can be usefully divided into questions about:

(1) intervals of defined boundaries, 

(2) intervals of defined probability mass, and 

(3) point estimates.

$~$

### Intervals of defined boundaries

Suppose I ask you for the posterior probability that the proportion of water is less than 0.5. Using the grid-approximate posterior, you can just add up all of the probabilities, where the corresponding parameter value is less than 0.5:

```{r intervals}

# add up posterior probability where p < 0.5
sum( posterior[ p_grid < 0.5 ] )

```

So let’s see how to perform the same calculation, using samples from the posterior. This approach does generalize to complex models with many parameters, and so you can use it everywhere. All you have to do is similarly add up all of the samples below 0.5, but also divide the resulting count by the total number of samples. In other words, find the frequency of parameter values below 0.5:

```{r samples}

sum(samples < 0.5)/ 1e4

# The answer is the posterior probability below a parameter value of 0.5.

```

And that’s nearly the same answer as the grid approximation provided, although your answer will not be exactly the same, because the exact samples you drew from the posterior will be different.

Using the same approach, you can ask how much posterior probability lies between 0.5 and 0.75:

```{r samples1}

sum(samples > 0.5 & samples < 0.75)/ 1e4

# So about 61% of the posterior probability lies between 0.5 and 0.75.

```

<span style="color:brown">***Counting with sum***</span>

In the R code examples just above, I used the function sum to effectively count up how many samples fulfill a logical criterion. Why does this work? It works because R internally converts a logical expression, like samples < 0.5, to a vector of TRUE and FALSE results, one for each element of samples, saying whether or not each element matches the criterion.Go ahead and enter samples < 0.5 on the R prompt, to see this for yourself. Then when you sum this vector of TRUE and FALSE, R counts each TRUE as 1 and each FALSE as 0. So it ends up counting how many TRUE values are in the vector, which is the same as the number of elements in samples that match the logical criterion.

$~$

### Intervals of defined mass

It is more common to see scientific journals reporting an interval of defined mass, usually known as a confidence interval. An interval of posterior probability, such as the ones we are working with, may instead be called a credible interval, although the terms may also be used interchangeably.

These posterior intervals report two parameter values that contain between them a specified amount of posterior probability, a probability mass. For this type of interval, it is easier to find the answer by using samples from the posterior than by using a grid approximation.

```{r example}

# Suppose for example you want to know the boundaries of the lower 80% posterior probability. 
# You know this interval starts at p = 0. To find out where it stops, think of the samples as data 
# and ask where the 80th percentile lies:
quantile(samples, 0.8)
# Lower 80% posterior probability exists below a parameter value of about 0.75. 

# For the middle 80% interval lies between the 10th percentile and the 90th percentile.
quantile(samples, c(0.1, 0.9))

# Intervals of this sort are called percentile intervals (PI).

```

<span style="color:brown"><span style="text-decoration:underline">**Percentile Intervals (PI)**</span>

- Intervals which assign equal probability mass to each tail

- Do a good job of communicating the shape of a distribution, as long as the distribution isn't too asymmetrical

- Not perfect for supporting inferences about which parameters are consistent with the data, see the figure and computation below:

```{r creat_sample2}

# This posterior is consistent with observing three waters in three tosses and a uniform (flat) prior. 
# It is highly skewed, having its maximum value at the boundary, p = 1. 
# You can compute it, via grid approximation, with:

# Create sample using grid approximation
p_grid <- seq(from = 0, to = 1, length.out = 1000)
prior <- rep(1, 1000)
likelihood <- dbinom(3, size = 3, prob = p_grid)
unstd.posterior <- prior * likelihood
posterior <- unstd.posterior/ sum(unstd.posterior)
samples2 <- sample(p_grid, size = 1e4, replace = TRUE, prob = posterior) # sample from posterior

# Compute the 50% percentile confidence interval from the samples with PI (part of rethinking)
PI(samples2, prob = 0.5)
# This interval assigns 25% of the probability mass above and below the interval. 
# So it provides the central 50% probability. But in this example, it ends up excluding 
# the most probable parameter values, near p = 1. So in terms of describing 
# the shape of the posterior distribution—which is really all these intervals are asked to do—
# the percentile interval can be misleading.
pi_50 <- PI(samples2, prob = 0.5)

# Create a data frame for plotting
df <- tibble(p_grid, posterior)

# Plot 50% PI 
df %>%
  ggplot(aes(x = p_grid, y = posterior)) +
  geom_ribbon(data = df %>% filter(p_grid > pi_50[1] & p_grid < pi_50[2]),
              aes(ymin = 0, ymax = posterior),
              fill = "purple", alpha = 0.5) +
  geom_line(color = "brown") +
  labs(subtitle = '50% Percentile interval',
       x = 'proportion of water (p)',
       y = 'density') +
  theme_bw()
# The posterior density here corresponds to a flat prior and observing 
# three water samples in three total tosses of the globe. 
# This interval assigns equal mass (25%) to both the left and right tail. 
# As a result, it omits the most probable parameter value, p = 1.

```


<span style="color:brown"><span style="text-decoration:underline">**Highest Posterior Density Interval (HPDI)**</span>

- The narrowest interval containing the specified probability mass

- If you think about it, there must be an infinite number of posterior intervals with the same mass. But if you want an interval that best represents the parameter values most consistent with the data, then you want the densest of these intervals- the HPDI.

``` {r HPDI_50}

# Create sample using grid approximation
p_grid <- seq(from = 0, to = 1, length.out = 1000)
prior <- rep(1, 1000)
likelihood <- dbinom(3, size = 3, prob = p_grid)
unstd.posterior <- prior * likelihood
posterior <- unstd.posterior/ sum(unstd.posterior)
samples2 <- sample(p_grid, size = 1e4, replace = TRUE, prob = posterior)

# Compute it from the samples with HPDI (part of rethinking)
HPDI(samples2, prob = 0.5)
# This interval captures the parameters with highest posterior probability, 
# as well as being noticeably narrower: 
# 0.16 in width rather than 0.23 for the percentile interval.
hpdi_50 <- HPDI(samples2, prob = 0.5)

# Create a data frame for plotting
df <- tibble(p_grid, posterior)

# Plot 50% PI 
df %>%
  ggplot(aes(x = p_grid, y = posterior)) +
  geom_ribbon(data = df %>% filter(p_grid > hpdi_50[1] & p_grid < hpdi_50[2]),
              aes(ymin = 0, ymax = posterior),
              fill = "purple", alpha = 0.5) +
  geom_line(color = "brown") +
  labs(subtitle = '50% Percentile interval',
       x = 'proportion of water (p)',
       y = 'density') +
  theme_bw()
# This interval finds the narrowest region with 50% of the posterior probability. 
# Such a region always includes the most probable parameter value.

```

$~$

<span style="color:brown"><span style="text-decoration:underline">**HPDI vs. PI**</span>

- The two types of intervals are very similar

- HPDI has some advantages over the PI
  - only look so different if the posterior distribution is highly skewed, otherwise nearly identical:

```{r HPDI,PI}

# Compare HPDI and PI

# Create samples using observation six waters in nine tosses
p_grid <- seq(from = 0, to = 1, length.out = 1000)
prior <- rep(1, 1000)
likelihood <- dbinom(6, size = 9, prob = p_grid)
posterior <- prior * likelihood
posterior <- posterior/sum(posterior)
samples3 <- sample(p_grid, size = 1e4, replace = TRUE, prob = posterior)

# Compute 80% HPDI and PI from samples
HPDI(samples3, prob = 0.8)
PI(samples3, prob = 0.8)

# Compute 95% HPDI and PI from samples
HPDI(samples3, prob = 0.95)
PI(samples3, prob = 0.95)

# Add name for 80% HPDI and PI
hpdi_80 <- HPDI(samples3, prob = 0.8)
pi_80 <- PI(samples3, prob = 0.8)

# Create a data frame for plotting
df <- tibble(p_grid, posterior)

# Plot 80% HPDI and PI on the same graph
df %>%
  ggplot(aes(x = p_grid, y = posterior)) +
  geom_ribbon(data = df %>% filter(p_grid > hpdi_80[1] & p_grid < hpdi_80[2]),
              aes(ymin = 0, ymax = posterior),
              fill = "blue", alpha = 0.5) +
  geom_ribbon(data = df %>% filter(p_grid > pi_80[1] & p_grid < pi_80[2]),
              aes(ymin = 0, ymax = posterior),
              fill = "green", alpha = 0.5) +
  geom_line(color = "red") +
  labs(subtitle = '80% HPDI and 80% PI',
       x = 'proportion of water (p)',
       y = 'density') +
  theme_bw() 

# Add name for 95% HPDI and PI 
hpdi_95 <- HPDI(samples3, prob = 0.95)
pi_95 <- PI(samples3, prob = 0.95)

# Plot 95% HPDI and PI on the same graph
df %>%
  ggplot(aes(x = p_grid, y = posterior)) +
  geom_ribbon(data = df %>% filter(p_grid > hpdi_95[1] & p_grid < hpdi_95[2]),
              aes(ymin = 0, ymax = posterior),
              fill = "brown", alpha = 0.5) +
  geom_ribbon(data = df %>% filter(p_grid > pi_95[1] & p_grid < pi_95[2]),
              aes(ymin = 0, ymax = posterior),
              fill = "yellow", alpha = 0.5) +
  geom_line(color = "grey") +
  labs(subtitle = '95% HPDI and 95% PI',
       x = 'proportion of water (p)',
       y = 'density') +
  theme_bw() 
 
 # When the posterior is bell shaped, it hardly matters which type of interval you use.

```

- HPDI also has some disadvantages
  - HPDI is more computationally intensive than PI
  - HPDI suffers from greater simulation variance (it is sensitive to how many samples you draw from the posterior.)
  
```
Overall, if the choice of interval type makes a big difference, then you shouldn’t be using intervals 
to summarize the posterior. Remember, the entire posterior distribution is the Bayesian estimate. It summarizes the relative plausibilities of each possible value of the parameter. Intervals of the 
distribution are just helpful for summarizing it. If choice of interval leads to different 
inferences, then you’d be better off just plotting the entire posterior distribution.

```
$~$

### Point estimates

The Bayesian parameter estimate is precisely the entire posterior distribution, which is not a single number, but instead a function that maps each unique parameter value onto a plausibility value. So really the most important thing to note is that you don’t have to choose a point estimate. It’s hardly ever necessary.

<span style="color:brown">***Which point estimate to choose if we must report: Mean, Median or Mode (MAP)?***</span>

```{r point_estimate}

# Use the globe tossing example where 3 water observations out of 3 tosses
p_grid <- seq(from = 0, to = 1, length.out = 1000)
prior <- rep(1, 1000)
likelihood <- dbinom(3, size = 3, prob = p_grid)
unstd.posterior <- prior * likelihood
posterior <- unstd.posterior/ sum(unstd.posterior)
samples2 <- sample(p_grid, size = 1e4, replace = TRUE, prob = posterior)

# Compute the MAP from posterior
# It is very common for scientists to report the parameter value with highest 
# posterior probability, a maximum a posteriori (MAP) estimate.
p_grid[ which.max(posterior) ]

# Compute MAP using samples from posterior
chainmode( samples2, adj = 0.01) # rethinking

map_value <- p_grid[ which.max(posterior) ]

# mean of the samples
mean(samples2)
mean_value <- mean(samples2)

# median of the samples
median(samples2)
median_value <- median(samples2)

# Create a data frame for plotting
df <- tibble(p_grid, posterior)

# Plot with lines for mean, median, and mode
df %>%
  ggplot(aes(x = p_grid, y = posterior)) +
  geom_line(color = "brown") +
  geom_ribbon(aes(ymin = 0, ymax = posterior), fill = "yellow") +
  geom_vline(xintercept = map_value, linetype = "dashed", color = "black") +
  geom_vline(xintercept = mean_value, linetype = "dashed", color = "black") +
  geom_vline(xintercept = median_value, linetype = "dashed", color = "black") +
  annotate("text", x = map_value, y = max(df$posterior), label = round(map_value, 2), 
           vjust = -0.5, hjust = 0, color = "black") +
  annotate("text", x = mean_value, y = max(df$posterior), label = round(mean_value, 2), 
           vjust = -0.5, hjust = 0, color = "black") +
  annotate("text", x = median_value, y = max(df$posterior), label = round(median_value, 2), 
           vjust = -0.5, hjust = 0, color = "black") +
  annotate("text", x = map_value, y = max(df$posterior), label = "mode", 
           vjust = -0.5, hjust = 2, color = "black", angle = 90) +
  annotate("text", x = mean_value, y = max(df$posterior), label = "mean", 
           vjust = -0.5, hjust = 2, color = "black", angle = 90) +
  annotate("text", x = median_value, y = max(df$posterior), label = "median",
           vjust = -0.5, hjust = 2, color = "black", angle = 90) +
  labs(subtitle = 'Posterior with Mean, Median, and Mode (MAP)',
       x = 'proportion of water (p)',
       y = 'density') +
  theme_bw() 

```

The value for median, mean, and mode (MAP) are different. In this case, ***use a loss function to estimate the cost associated with using any particular point estimate.*** 

<span style="color:brown"><span style="text-decoration:underline">**Using loss function**</span>

- Different loss functions imply different point estimates

- The loss is proportional to the distance of your decision from the true value
  - Loss $\propto$ d - p, where d is the decision and p is the correct answer or true value 

\n

-  The parameter value that maximizes expected winnings (minimizes expected loss) is the median of the posterior distribution

- Calculating expected loss for any given decision means using the posterior to average over our uncertainty in the true value (unknown in most cases)

``` 
In order to decide upon a point estimate, a single-value summary of the posterior distribution, we need 
to pick a loss function. Different loss functions nominate different point estimates. The two most 
common examples are the absolute loss (d - p), which leads to the median as the point estimate, and the quadratic loss (d − p)2, which leads to the posterior mean (mean(samples)) as the point estimate. When 
the posterior distribution is symmetrical and normal-looking, then the median and mean converge to the same point.

```

```{r loss}
# If our decision is p = 0.5,
# compute the weighted average loss, where each loss is weighted by its 
# corresponding posterior probability
sum(posterior * abs(0.5 - p_grid))

# repeating this calculation for every possible decision, using the function sapply
loss <- sapply( p_grid , function(d) sum( posterior*abs( d - p_grid ) ) )
# Now the symbol loss contains a list of loss values, one for each possible 
# decision, corresponding the values in p_grid

# Find the parameter value that minimizes the loss
p_grid[ which.min(loss)]
# And this is actually the posterior median, the parameter value that splits 
# the posterior density such that half of the mass is above it 
# and half below it

# Check with posterior median
median(samples2)
# The answer match with the value that minimizes the loss

# Plot the loss function
# Find x, y values for minimum loss
min_loss_x <- p_grid[which.min(loss)]
min_loss_y <- loss[which.min(loss)]

# Create data frame and plot 
df <- tibble(p_grid, loss, posterior)

df %>% 
  ggplot(aes(x = p_grid)) +
  geom_ribbon(aes(ymin = 0, ymax = loss), fill = 'grey75') +
  geom_point(aes(x = min_loss_x, y = min_loss_y), size = 3, shape = 21, color = 'black') +
  geom_vline(xintercept = map_value, linetype = "dashed", color = "grey") +
  geom_vline(xintercept = mean_value, linetype = "dashed", color = "grey") +
  geom_vline(xintercept = median_value, linetype = "dashed", color = "grey") +
   annotate("text", x = map_value, y = max(df$posterior), label = "mode", 
            vjust = -0.5, hjust = -5, color = "black", angle = 90) +
  annotate("text", x = mean_value, y = max(df$posterior), label = "mean", 
           vjust = -0.5, hjust = -5, color = "black", angle = 90) +
  annotate("text", x = median_value, y = max(df$posterior), label = "median", 
           vjust = -0.5, hjust = -5, color = "black", angle = 90) +
  annotate("text", x = min_loss_x, y = min_loss_y, label = "value that \nminimizes loss", 
           vjust = 1.2, hjust = 0.5, color = "black") +
  labs(x = 'decision',
       y = 'expected proportional loss') +
  theme(panel.grid = element_blank())

# Expected loss under the rule that loss is proportional to absolute distance 
# of decision (horizontal axis) from the true value. The point marks the value 
# of p that minimizes the expected loss, the posterior median.

```

$~$

## Sampling to simulate prediction

Samples from posterior ease simulation of the model’s implied observations. Generating implied observations from a model is useful for at least four distinct reasons.

(1) Model checking- to check whether the fit worked correctly and to investigate model behavior

(2) Software validation- simulate observations under a known model and then attempt to recover the values of the parameters the data were simulated under

(3) Research design- simulate observations from hypothesis to evaluate whether the research design can be effective

(4) Forecasting- can be useful as applied prediction, but also for model criticism and revision

$~$

### Dummy data (simulated data)

Likelihood functions work in both directions. Given a realized observation, the likelihood function says how plausible the observation is. And given only the parameters, the likelihood defines a distribution of possible observations that we can sample from, to simulate observation. In this way, Bayesian models are always generative, capable of simulating predictions. 

With the globe tossing model, the dummy data arises from a binomial likelihood:

Pr(w|n,p) = $\frac{n!}{w!(n - w)!}$p^w^ (1 - p)^n-w^

where w is an observed count of “water” and n is the number of tosses. Suppose n = 2, two tosses of the globe. Then there are only three possible observations: 0 water, 1 water, 2 water. You can quickly compute the likelihood of each, for any given value of p. Let’s use p = 0.7, which is just about the true proportion of water on the Earth:

```{r likelihood}

dbinom(0:2, size = 2, prob = 0.7)
# 9% chance of observing w = 0, a 42% chance of w = 1, and a 49% chance of w = 2

```

Now we’re going to simulate observations, using these likelihoods. So a single dummy data observation of w can be sampled with:

```{r rbinom}

rbinom(1, size = 2, prob = 0.7)

```

The “r” in rbinom stands for “random.” It can also generate more than one simulation at a
time. A set of 10 simulations can be made by:

```{r rbinom1}

rbinom(10, size = 2, prob = 0.7)

```

Let’s generate 100,000 dummy observations, just to verify that each value (0, 1, or 2) appears in proportion to its likelihood:

```{r check}

dummy_w <- rbinom(1e5, size = 2, prob = 0.7)

table(dummy_w) /1e5

# values are very close to the likelihood values calculated above

```

Now let’s simulate the same sample size as before, 9 tosses.

```{r 9tosses}

dummy_w_9 <- rbinom(1e5, size = 9, prob = 0.7)
table(dummy_w_9)/ 1e5
simplehist(dummy_w_9, xlab = "dummy water count")
# Distribution of simulated sample observations from 9 tosses of the globe. 
# These samples assume the proportion of water is 0.7.
# Notice that most of the time the expected observation does not contain water 
# in its true proportion, 0.7. That’s the nature of observation: There is a 
# one-to-many relationship between data and data-generating processes.

```

Test with sample size = 40 and probability p = 0.5

```{r 40tosses}

dummy_w_40 <- rbinom(1e5, size = 40, prob = 0.5)
simplehist(dummy_w_40, xlab = "dummy water count")

```

<span style="color:brown"><span style="text-decoration:underline">**Sampling distribution**</span>

In this book, inference about parameters is never done directly through a sampling distribution. The posterior distribution is not sampled, but deduced logically. Then samples can be drawn from the posterior, to aid in inference. In neither case is “sampling” a physical act. In both cases, it’s just a mathematical device and produces only small world numbers.

$~$

### Model checking

- Ensures the model fitting worked correctly

- Evaluate the adequacy of a model for some purpose

$~$

#### Did the software work?

- Use retrodictions (implied predictions) to check whether the software worked- by checking for correspondence between implied predictions and the data used to fit the model

- An exact match is neither expected nor desired, but when there is no correspondence at all, it probably means the software did something wrong.

- There is no way to really be sure that software works correctly. Even when the retrodictions correspond to the observed data, there may be subtle mistakes.

$~$

#### Is the model adequate?

- Look for aspects of the data that are not well described by the model’s expectations

- The goal is not to test whether the model’s assumptions are “true,” because all models are false

- The goal is to assess exactly how the model fails to describe the data, as a path towards model comprehension, revision, and improvement

<span style="color:brown">***Basic model checks using simulated observations for the globe tossing model***</span>

The implied predictions of the model are uncertain in two ways, and we’d like to propagate the parameter uncertainty as we evaluate the implied predictions, i.e. get a posterior predictive distribution.

- Observation uncertainty
  - There is uncertainty in the predicted observations, because even if you know p with certainty, you won’t know the next globe toss with certainty (unless p = 0 or p = 1).
  
\n

- Uncertainty about p 
  - Since there is uncertainty about p, there is uncertainty about everything that depends upon p.
  - The uncertainty in p will interact with the sampling variation, when we try to assess what the model tells us about outcomes.
  
$~$

<span style="color:brown"><span style="text-decoration:underline">**Posterior predictive distribution**</span>

For each possible value of the parameter p, there is an implied distribution of outcomes. So if you were to compute the sampling distribution of outcomes at each value of p, then you could average all of these prediction distributions together, using the posterior probabilities of each value of p, to get a posterior predictive distribution.

- The sampling distributions for all values of p are combined, using the posterior probabilities to compute the weighted average frequency of each possible observation.

- It incorporates all of the uncertainty embodied in the posterior distribution for the parameter p. As a result, it is honest- the distribution may be narrower (overconfidence) if we discard uncertainty about the parameters. 

<span style="color:purple">***Calculate posterior predictive distribution***</span>

```{r PPD}

# To simulate predicted observations for a single value of p, say p = 0.6, 
# you can use rbinom to generate random binomial samples:
w <- rbinom(1e4, size = 9, prob = 0.6)
simplehist(w, xlab = "number of water samples")

# All you need to propagate parameter uncertainty into these predictions is 
# replace the value 0.6 with samples from the posterior:
w <- rbinom(1e4, size = 9, prob = samples)
simplehist(w, xlab = "number of water samples", main = "PPD")
# For each sampled value, a random binomial observation is generated. 
# Since the sampled values appear in proportion to their posterior probabilities,
# the resulting simulated observations are averaged over the posterior. 

```

```
In your own modeling, you’ll have to imagine aspects of the data that are relevant in your context, for your purposes, i.e. inspect the data in new ways, e.g. correlation between samples that may be bias. 

```

$~$

## Practice

### Easy

```{r practice_sample}

p_grid <- seq( from=0 , to=1 , length.out=10000 )
prior <- rep( 1 , 10000 )
likelihood <- dbinom( 6 , size=9 , prob=p_grid )
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
set.seed(100)
samples <- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE )

```

3E1. How much posterior probability lies below p = 0.2?

```{r 3E1}

sum(samples < 0.2)/ 1e4
# 5 x 10^-4^ % of posterior probability lies below p = 0.2.

```

$~$

3E2. How much posterior probability lies above p = 0.8?

```{r 3E2}

sum(samples > 0.8)/ 1e4
# 12% of posterior probability lies between p = 0.2 and p = 0.8

```

$~$

3E3. How much posterior probability lies between p = 0.2 and p = 0.8? 

```{r 3E3}

sum(samples > 0.2 & samples < 0.8)/ 1e4
# 88% of posterior probability lies between p = 0.2 and p = 0.8

```

$~$
  
3E4. 20% of the posterior probability lies below which value of p?

``` {r 3E4}

quantile(samples, 0.2)
# 20% of the posterior probability lies below p = 0.51

```

3E5. 20% of the posterior probability lies above which value of p?

```{r 3E5}
# 20% is the quantile in the upper end of distribution,
# hence we find quantile value below 80%
quantile(samples, 0.8)
# 20% of the posterior probability lies above p = 0.76

```

$~$

3E6. Which values of p contain the narrowest interval equal to 66% of the posterior probability?

``` {r 3E6}
HPDI(samples, 0.66)
# p interval of 0.5-0.8 contain the narrowest interval equal to 66% of 
# the posterior probability

```

$~$

3E7. Which values of p contain 66% of the posterior probability, assuming equal posterior probability both below and above the interval?

```{r 3E7}
PI(samples, 0.66)

```

$~$

### Medium

3M1. Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before.

```{r 3M1}
p_grid <- seq(from = 0, to = 1, length.out = 10000)
prior <- rep(1, 10000)
likelihood <- dbinom(8, size = 15, prob = p_grid)
unstd.posterior <- likelihood * prior
posterior <- unstd.posterior/ sum(unstd.posterior)

plot(posterior~p_grid, type ="l")

```

$~$

3M2. Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for p.

```{r 3M2}

p_grid <- seq(from = 0, to = 1, length.out = 10000)
prior <- rep(1, 10000)
likelihood <- dbinom(8, size = 15, prob = p_grid)
unstd.posterior <- likelihood * prior
posterior <- unstd.posterior/ sum(unstd.posterior)
set.seed(101)
sample3m2 <- sample(p_grid, size = 1e4, replace = TRUE, prob = posterior)

# Calculate 90% HPDI for p 
HPDI(sample3m2, 0.9)

```

$~$

3M3. Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in p. What is the probability of observing 8 water in 15 tosses?

```{r 3M3}

w <- rbinom(1e4, size = 15, prob = sample3m2)
simplehist(w, xlab= "number of water observations")
sum(w == 8)/1e4


```

$~$

3M4. Using the posterior distribution constructed from the new (8/15) data,now calculate the probability of observing 6 water in 9 tosses.

```{r 3M4}

w <- rbinom(1e4, size = 9, prob = sample3m2)
table(w)/ 1e4
# Probability of 6w in 9n is 18%

# Generate the answer only
table(w == 6)/ 1e4

```

$~$

3M5. Start over at 3M1, but now use a prior that is zero below p = 0.5 and a constant above p = 0.5. This corresponds to prior information that a majority of the Earth’s surface is water. Repeat each problem above and compare the inferences. What difference does the better prior make? If it helps, compare inferences (using both priors) to the true value p = 0.7.

*For 3M1 and 3M3*
```{r 3M51}

# Set up a 1x2 layout for side-by-side plots
par(mfrow = c(1, 2))

# First plot
p_grid <- seq(from = 0, to = 1, length.out = 10000)
prior <- rep(1, 10000)
likelihood <- dbinom(8, size = 15, prob = p_grid)
unstd.posterior <- likelihood * prior
posterior <- unstd.posterior / sum(unstd.posterior)
plot(posterior ~ p_grid, type = "l", col = "blue", lty = 1, 
     xlab = "Probability of water", ylab = "Posterior distribution",
     main = "Prior = 1")

# Second plot
w <- rbinom(1e4, size = 15, prob = sample3m2)
simplehist(w, col = "black", lty = 1,
     xlab = "Count of water observations", ylab = "Frequency",
     main = "Prior = 1")

# Set up a 1x2 layout for side-by-side plots
par(mfrow = c(1, 2))

# Third plot
p_grid <- seq(from = 0, to = 1, length.out = 10000)
prior <- ifelse(p_grid < 0.5, 0, 1)
likelihood <- dbinom(8, size = 15, prob = p_grid)
unstd.posterior <- likelihood * prior
posterior <- unstd.posterior / sum(unstd.posterior)
set.seed(102)
sample3m5 <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)
plot(posterior ~ p_grid, type = "l", col = "red", lty = 1, 
     xlab = "Probability of water", ylab = "Posterior distribution",
     main = "Prior = 0 or 1")

# Fourth plot
w <- rbinom(1e4, size = 15, prob = sample3m5)
simplehist(w, col = "black", lty = 1,
     xlab = "Count of water observations", ylab = "Frequency",
     main = "Prior = 0 or 1")

# When the prior becomes more informative, there is a mismatch between prediction and observed data

```

$~$

*For 3M2*
```{r 3M52}

# Calculate 90% HPDI for p when prior = 1
HPDI(sample3m2, 0.9)

# Calculate 90% HPDI for p when prior = 0 or 1
HPDI(sample3m5, 0.9)
# narrower interval for 90% HPDI when prior is more informative

```
$~$

*For 3M4*

```{r 3M54}

# % Probability of 6w in 9n when prior = 1
w <- rbinom(1e4, size = 9, prob = sample3m2)
table(w == 6)/ 1e4

# % Probability of 6w in 9n when prior = 0 or 1
x <- rbinom(1e4, size = 9, prob = sample3m5)
table(w ==6)/ 1e4
# No difference in probability percentage 

```
$~$

### Hard

3H1. Using grid approximation, compute the posterior distribution for the probability of a birth being a boy. Assume a uniform prior probability. Which parameter value maximizes the posterior probability?

```{r 3H1}

# Load homework data
data(homeworkch3)

# Total number of boys born across all this birth
sum(birth1) + sum(birth2)

# Compute posterior distribution for a birth being a boy
p_grid <- seq(from = 0, to = 1, length.out = 200)
prior <- rep(1, 200)
likelihood <- dbinom(111, size = 200, prob = p_grid)
unstd.posterior <- prior * likelihood
posterior <- unstd.posterior/ sum(unstd.posterior)

# Plot posterior with abline
plot(posterior~p_grid, type = "l")
abline(v = 0.5, lty = 2, col = "red")

# Find parameter value that maximizes posterior probability
p_grid[which.max(posterior)]

# Show parameter with max posterior probability in plot
v <- p_grid[which.max(posterior)]
plot(posterior~p_grid, type = "l")
abline(v = 0.5, lty = 2, col = "red")
abline(v = v, lty = 2, col = "blue")
legend("topright", legend = c("p = 0.5", "p = 0.553"), 
       col = c("red", "blue"), lty = c(2, 2))

# OR
# Use loss function to calculate parameter value that maximizes posterior probability
# Add sample
set.seed(102)
sample3h1 <- sample(p_grid, size = 1e4, prob = posterior, replace = TRUE)

# Calculate mode (MAP) value
map_value <- p_grid[which.max(posterior)]
map_value

# Calculate mean value
mean_value <- mean(sample3h1)
mean_value

# Calculate median value
median_value <- median(sample3h1)
median_value

# Calculate loss
loss <- sapply(p_grid , function(d) sum(posterior * abs(d - p_grid)))

# Plot the loss function
# Find x, y values for minimum loss
min_loss_x <- p_grid[which.min(loss)]
min_loss_y <- loss[which.min(loss)]

# Create data frame and plot 
df <- tibble(p_grid, loss, posterior)

df %>% 
  ggplot(aes(x = p_grid)) +
  geom_ribbon(aes(ymin = 0, ymax = loss), fill = 'lightblue') +
  geom_point(aes(x = min_loss_x, y = min_loss_y), size = 3, shape = 21, color = 'black') +
  geom_vline(xintercept = map_value, linetype = "dashed", color = "grey") +
  geom_vline(xintercept = mean_value, linetype = "dashed", color = "grey") +
  geom_vline(xintercept = median_value, linetype = "dashed", color = "grey") +
   annotate("text", x = map_value, y = max(df$posterior), label = "mode", 
            vjust = -0.5, hjust = -3, color = "black", angle = 90) +
  annotate("text", x = mean_value, y = max(df$posterior), label = "mean", 
           vjust = -0.5, hjust = -5, color = "black", angle = 90) +
  annotate("text", x = median_value, y = max(df$posterior), label = "median", 
           vjust = -0.5, hjust = -5, color = "black", angle = 90) +
  annotate("text", x = min_loss_x, y = min_loss_y, label = "value that minimizes loss", 
           vjust = 1.6, hjust = 0.5, color = "black") +
  labs(x = 'parameter value',
       y = 'expected proportional loss') +
  theme(panel.grid = element_blank())

# The parameter value that maximizes probability is 0.553 

```

$~$

3H2. Using the sample function, draw 10,000 random parameter values from the posterior distribution you calculated above. Use these samples to estimate the 50%, 89%, and 97% highest posterior density intervals.

```{r 3H2}

# Calculate 50% HPDI
HPDI(sample3h1, prob = 0.5)

# Calculate 89% HPDI
HPDI(sample3h1, prob = 0.89)

# Calculate 97% HPDI
HPDI(sample3h1, prob = 0.97)

```
$~$

3H3. Use rbinom to simulate 10,000 replicates of 200 births. You should end up with 10,000 numbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). There are many good ways to visualize the simulations, but the dens command (part of the rethinking package) is probably the easiest way in this case. Does it look like the model fits the data well? That is, does the distribution of predictions include the actual observation as a central, likely outcome?

```{r 3H3}
# Use rbinom to simulate 10,000 replicates of 200 births
replicate_w <- rbinom(10000, size = 200, prob = sample3h1)

# Compare predictive posterior distribution and actual count using dens function
dens(replicate_w, adj = 0.1, col = "darkgreen", 
     main = "PPD with 200 grids, 1e4 samples, 1e4 observations")
abline(v = 111, col = "red")
# PPD is not an exact fit- is that because i used 200 grids only?

# Try increasing grid size
p_grid <- seq(from = 0, to = 1, length.out = 10000)
prior <- rep(1, 10000)
likelihood <- dbinom(111, size = 200, prob = p_grid)
unstd.posterior <- likelihood * prior
posterior_w <- unstd.posterior/ sum(unstd.posterior)
set.seed(103)
sample_grid <- sample(p_grid, size = 10000, replace = TRUE, prob = posterior_w)
replicate_w1 <- rbinom(10000, size = 200, prob = sample_grid)
dens(replicate_w1, adj = 0.1, col = "darkgreen", 
     main = "PPD with 1e4 grids, 1e4 samples, 1e4 observations")
abline(v = 111, col = "red")

# Try with codes from answer key
data(homeworkch3)
boys <- sum(birth1) + sum(birth2)
p <- seq( from=0 , to=1 , length.out=1000 )
prior <- rep(1,length(p))
likelihood <- dbinom( boys , size=200 , prob=p )
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
p.samples <- sample( p , size=10000 , replace=TRUE , prob=posterior )
bsim <- rbinom( 10000 , size=200 , prob=p.samples )
dens( bsim , adj=0.1, main = "Given answer: 1000 grids, 1e4 samples, 1e4 observations")
abline( v=sum(birth1)+sum(birth2) , col="red" )
# Can't get the exact fit like the graph in answer key even when I just c/p the codes.

# Try increasing sample with 1e5 grids
p_grid <- seq(from = 0, to = 1, length.out = 1e5)
prior <- rep(1, 1e5)
likelihood <- dbinom(111, size = 200, prob = p_grid)
unstd.posterior <- likelihood * prior
posterior_w <- unstd.posterior/ sum(unstd.posterior)
set.seed(10)
sample_grid <- sample(p_grid, size = 1e5, replace = TRUE, prob = posterior_w)
replicate_w1 <- rbinom(1e5, size = 200, prob = sample_grid)
dens(replicate_w1, adj = 0.1, col = "darkgreen", 
     main = "PPD with 1e5 grids, 1e5 samples, 1e5 observations")
abline(v = 111, col = "red")

# Try with 1e4 observations to see difference
p_grid <- seq(from = 0, to = 1, length.out = 1e5)
prior <- rep(1, 1e5)
likelihood <- dbinom(111, size = 200, prob = p_grid)
unstd.posterior <- likelihood * prior
posterior_w <- unstd.posterior/ sum(unstd.posterior)
set.seed(10)
sample_grid <- sample(p_grid, size = 1e5, replace = TRUE, prob = posterior_w)
replicate_w1 <- rbinom(1e4, size = 200, prob = sample_grid)
dens(replicate_w1, adj = 0.1, col = "darkgreen", 
     main = "PPD with 1e5 grids, 1e5 samples, 1e4 observations")
abline(v = 111, col = "red")

# All graphs have slightly different peaks. Is it because of variations in 
# grid size/ sample size/ observation size or variations in draw?
# But PPDs' peaks are within ± 3 of actual counts. This only shows the fit of data-shows model works
# Doesn't show if a model is good or not- whether reflect the real world. 
# So, it doesn't matter if slightly different fit, I think. 

```

$~$

3H4. Now compare 10,000 counts of boys from 100 simulated first borns only to the number of boys in the first births, birth1. How does the model look in this light?

```{r 3H4}
# Calculate posterior for first born boys
first_born_boy <- sum(birth1)
p_grid_3h4 <- seq(from = 0, to = 1, length.out = 1000)
prior_3h4 <- rep(1, 1000)
likelihood_3h4 <- dbinom(first_born_boy, size = 100, prob = p_grid_3h4)
unstd.posterior_3h4 <- prior_3h4 * likelihood_3h4
posterior_3h4 <- unstd.posterior_3h4/ sum(unstd.posterior_3h4)

# Create samples
set.seed(111)
sample_3h4 <- sample(p_grid_3h4, size = 1e4, replace = TRUE, prob = posterior_3h4)

# Create observations
w_3h4 <- rbinom(1e4, size = 100, prob = sample_3h4)

# Compare PPD with actual count
dens(w_3h4, adj = 0.1, col = "black")
abline(v = first_born_boy, col = "green")

# Let's try increasing the grid to 1e5
p_grid_3h4 <- seq(from = 0, to = 1, length.out = 1e5)
prior_3h4 <- rep(1, 1e5)
likelihood_3h4 <- dbinom(first_born_boy, size = 100, prob = p_grid_3h4)
unstd.posterior_3h4 <- prior_3h4 * likelihood_3h4
posterior_3h4 <- unstd.posterior_3h4/ sum(unstd.posterior_3h4)

# Create samples
set.seed(111)
sample_3h4 <- sample(p_grid_3h4, size = 1e4, replace = TRUE, prob = posterior_3h4)

# Create observations
w_3h4 <- rbinom(1e4, size = 100, prob = sample_3h4)

# Compare PPD with actual count
dens(w_3h4, adj = 0.1, col = "black")
abline(v = first_born_boy, col = "green")
# Better fit with increased grid size. 

# If we use the same posterior distribution (given answer)
w_3h4 <- rbinom(1e4, size = 100, prob = sample3h1)
dens(w_3h4, adj = 0.1, col = "black")
abline(v = first_born_boy, col = "red")
# So, a new posterior with new conditions- changes in number of actual 
# counts and total counts- will make the model fit the data. I think this is
# caused by simulation not exactly halving the observations, i.e., we gave
# no command that the observation is 51 for 100.
# Given answer: Not bad, but not right on center now. The frequency of boys 
# among first borns is a little less than the model tends to predict. 
# The model doesn’t have a problem accounting for this variation though, 
# as the red line is well within density of simulated data.

```

$~$

3H5. The model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first borns. Compare 10,000 simulated counts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data?

```{r 3H5}

# Count number of second born boys when first borns are girl
second_boy_first_girl <- sum(birth1 == 0 & birth2 == 1)

# Count the number of first born girls
first_girl <- sum(birth1 == 0)

# Simulate observations for total number of second births when first births are girls
w_3h5 <- rbinom( 10000 , size = first_girl , prob = sample3h1 )
dens(w_3h5, adj = 0.1, col = "black")
abline(v = second_boy_first_girl , col = "green" )
# The sex of first and second birth may not be independent. 

# Run codes in given answer
# Create a new vector of birth 2 when birth 1 is 0
b01 <- birth2[birth1==0]

# Simulate 1e4 observations for new vector size using samples from model posterior
b01sim <- rbinom( 10000 , size=length(b01) , prob=p.samples )
dens(b01sim,adj=0.1)

# Add a line for actual count of second born boys when first borns are girls
abline( v=sum(b01) , col="red" )

```

$~$

# Chapter 4: Linear Models

Under a probability interpretation, which is necessary for Bayesian work, linear regression uses a Gaussian (normal) distribution to describe our golem’s uncertainty about some measurement of interest.

## Why normal distributions are normal

A thousand friends line up on the halfway line of a soccer field. Each flips a coin 16 times- moves a step to the left if the coin comes up head and to the right if tail. Then, measure the distance of each person from the halfway line. "The distances will be distributed in approximately normal, or Gaussian, fashion. This is true even though the underlying distribution is binomial. It does this because *there are so many more possible ways to realize a sequence of left-right steps that sums to zero. There are slightly fewer ways to realize a sequence that ends up one step left or right of zero, and so on, with the number of possible sequences declining in the characteristic bell curve of the normal distribution*."

### Normal by addition 

Simulate the above experiment in R:

```{r c41}

set.seed(01)
# To simulate this, we generate for each person a list of 16 random numbers between −1 and 1. 
# These are the individual steps. Then we add these steps together to get the position after 16 steps. 
# Then we need to replicate this procedure 1000 times.
pos <- replicate(1000, sum(runif(16, -1, 1))) 
# runif to generate random deviates following the uniform distribution 

hist(pos)
plot(density(pos))

  # Plot the density for 4 steps, 8 steps, and 16 steps
  par(mfrow = c(1, 4))
  
  # Plot for 4 steps
  pos4 <- replicate(1000, sum(runif(4, -1, 1)))
  dens(pos4, main = "4 steps", norm.comp = TRUE, col = "blue")
  
  # Plot for 8 steps
  pos8 <- replicate(1000, sum(runif(8, -1, 1)))
  dens(pos8, main = "8 steps", norm.comp = TRUE, col = "blue")
  
  # Plot for 16 steps
  dens(pos, main = "16 steps", norm.comp = TRUE, col = "blue")
  
  # Plot for 100 steps
  pos100 <- replicate(1000, sum(runif(100, -1, 1)))
  dens(pos100, main = "100 steps", norm.comp = TRUE, col = "blue")

# The more steps are taken, the closer the match between the real empirical distribution 
# of positions and the ideal normal distribution.
# After 16 steps, it has already taken on a familiar “bell” curve of the Gaussian distribution 
# emerging from the randomness, i.e., the distribution of positions is stabilizing on the Gaussian. 
# But they all look bell shape to me. 


```

$~$

<span style="color:purple">***Why addition should result in a bell curve of sums?***</span>

- Whatever the average value of the source distribution, each sample from it can be thought of as a fluctuation from that average value. 

- When we begin to add these fluctuations together, they also begin to cancel one another out. A large positive fluctuation will cancel a large negative one. 

- The more terms in the sum, the more chances for each fluctuation to be canceled by another, or by a series of smaller ones in the opposite direction. So eventually the most likely sum, in the sense that there are the most ways to realize it, will be a sum in which every fluctuation is canceled by another, a sum of zero (relative to the mean).

- It doesn’t matter what shape the underlying distribution possesses- uniform or anything else. Depending upon the underlying distribution, the convergence might be slow (often rapid), but it will be inevitable.

$~$

### Normal by multiplication

Simulate growth multiplication of an organism:

```{r c42}

# Sample 12 random numbers between 1.0 and 1.1, each representing a proportional 
# increase in growth. Thus 1.0 means no additional growth and 1.1 means a 10% increase. 
# The product of all 12 is computed and returned as output.
# prod returns the product of all the values present in its arguments.
prod(1 + runif(12, 0, 0.1)) 

# Generate 10,000 random products
growth <- replicate(10000, prod(1 + runif(12, 0, 0.1)))
dens(growth, norm.comp = TRUE, col = "darkred") #norm.comp overlay normal density comparison if true

# Small effects that multiply together are approximately additive, and so 
# they also tend to stabilize on Gaussian distributions.  For example, if there
# are two loci with alleles increasing growth by 10% each, the product is: 1.1 × 1.1 = 1.21
# The smaller the effect of each locus, the better this additive approximation will be.

# Verify this effect by comparing big vs. small increase in percent value
big <- replicate(10000, prod(1 + runif(12, 0, 0.5)))
small <- replicate(10000, prod(1 + runif(12, 0, 0.01)))

# Plot the distributions
par(mfrow = c(1, 2))
dens(big, col = "blue", norm.comp = TRUE, main = "Big effect")
dens(small, col = "red", norm.comp = TRUE, main = "Small effect")
# The interacting growth deviations, as long as they are sufficiently small, 
# converge to a Gaussian distribution- extends well beyond purely additive interactions.

```

$~$

### Normal by log-multiplication 

Large deviates that are multiplied together do not produce Gaussian distributions, but they do tend to produce Gaussian distributions on the log scale. For example:

```{r c43}
 
log.big <- replicate( 10000 , log(prod(1 + runif(12,0,0.5))) )
dens(log.big, norm.comp = TRUE, col = "blue", main = "Big effect on log scale")
# We get the Gaussian distribution back, because adding logs is equivalent to multiplying 
# the original numbers. So even multiplicative interactions of large deviations can produce 
# Gaussian distributions, once we measure the outcomes on the log scale. 

```

$~$

### Using Guassian distributions

The justifications for using the Gaussian distribution- as a skeleton for hypotheses, building up models of measurements as aggregations of normal distributions- fall into two broad categories: 

(1) ontological justification
    - Repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process, aside from mean and spread. One consequence of this is that statistical models based on Gaussian distributions cannot reliably identify micro-process. But it also means that these models can do useful work, even when they cannot identify process.
    
\n

(2) epistemological justification
      - Gaussian distribution is the most natural expression of our state of ignorance, because if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways and does not introduce any new assumptions. It is the least surprising and least informative assumption to make. In this way, the Gaussian is the distribution most consistent with our assumptions. Or rather, it is the most consistent with our golem’s assumptions.

$~$

<span style="color:brown"><span style="text-decoration:underline">**Gaussian distribution**</span>

- The probability density of some value y, given a Gaussian (normal) distribution with mean $\mu$ and standard deviation $\sigma$, is:

$$p(y|\mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(- \frac{(y-\mu)^2}{2\sigma^2}\right)$$

- (y − $\mu$)^2^ gives the normal distribution its fundamental shape, a quadratic shape. Once you exponentiate the quadratic shape, you get the classic bell curve. The rest of it just scales and standardizes the distribution so that it sums to one, as all probability distributions must. But an expression as simple as exp(−y^2^) yields the Gaussian prototype.

- The value y in the Gaussian distribution can be any continuous value, as Gussian is a continuous distribution. 

- The binomial, in contrast, requires integers. Probability distributions with only discrete outcomes, like the binomial, are usually called *probability mass functions* and denoted *Pr*. Continuous ones like the Gaussian are called *probability density functions*, denoted with *p* or just plain old *f*. 

- For mathematical reasons, probability densities, but not masses, can be greater than 1. For example:

```{r c44}

# Calculate p(0|0, 0.1)
dnorm(0, 0, 0.1)

# Probability density is the rate of change in cumulative probability. 
# So where cumulative probability is increasing rapidly, density can easily exceed 1. 
# But if we calculate the area under the density function, it will never exceed 1. 
# Such areas are also called probability mass.

```

- The Gaussian distribution is routinely seen without $\sigma$ but with another parameter, $\tau$ . The parameter $\tau$ in this context is usually called precision and defined as $\tau$ = 1/$\sigma$^2^. This change of parameters gives us the equivalent formula (just substitute $\sigma$ = 1/ $\sqrt\tau$):

$$p(y|\mu, \tau) = \sqrt\frac{\tau}{{2\pi}} \exp\left(- \frac{1}{2}\tau(y - \mu)^2 \right)$$

$~$

## A language for describing models

(1) First, we recognize a set of measurements that we hope to predict or understand, the *outcome* variable or variables.

(2) For each of these outcome variables, we define a likelihood distribution that defines the plausibility of individual observations. In linear regression, this distribution is always Gaussian.

(3) Then we recognize a set of other measurements that we hope to use to predict or understand the outcome. Call these *predictor* variables.

(4) We relate the exact shape of the likelihood distribution—its precise location and variance and other aspects of its shape, if it has them—to the predictor variables. In choosing a way to relate the predictors to the outcomes, we are forced to name and define all of the parameters of the model.

(5) Finally, we choose priors for all of the parameters in the model. These priors define the initial information state of the model, before seeing the data.

- After all these decisions are made, summarize the model: 


$$
\text{outcome}_i \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i = \beta \times \text{predictor}_i \\
\beta \sim \text{Normal}(0, 10) \\
\sigma \sim \text{HalfCauchy}(0, 1)
$$

<div style="border: 1px solid #999; padding: 10px; background-color: #f9f9f9; margin: 10px 0;">

outcome~i~                    = outcome variable for the i^th^ observation

Normal($\mu$~i~, $\sigma$)    = normal distribution with mean $\mu$~i~ and standard deviation $\sigma$

$\mu$~i~    = mean ($\mu$~i~) of the normal distribution for the i^th^ observation

$\beta\times\text{predictor}$ = the product of a coefficient ($\beta$) and a predictor variable (predictor~i~)


</div>

$~$

### Re-describing the globe tossing model

The model:

$$\text{w} \sim \text{Binomial(n, p)}$$
$$\text{p} \sim \text{Uniform(0, 1)}$$

```
w   = observed count of water samples
n   = total number of samples
p   = proportion of water on actual globe

The count w is distributed binomially with sample size n and probability p. 
The prior for p is assumed to be uniform between zero and one. 

In these models, the first line defines the likelihood function used in Bayes’ theorem. 
The other line defines priors. Both of the lines in this model are stochastic, as 
indicated by the ∼ symbol. A stochastic relationship is just a mapping of a variable 
or parameter onto a distribution. It is stochastic because no single instance of the 
variable on the left is known with certainty. Instead, the mapping is probabilistic: 
Some values are more plausible than others, but very many different values are 
plausible under any model. 

```

$~$

<span style="color:purple">***From model definition to Bayes' theorem***</span>

To relate the mathematical format above to Bayes’ theorem, you could use the model definition to define the posterior distribution:

$$\text{Pr(p|w,n)} = \frac{\text{Binomial(w|n,p) Uniform(p|0,1)}}{\int \text{Binomial(w|n,p) Uniform(p|0,1)dp}}$$

$~$

$$\text{posterior probability of any particular value of p} = \frac{likelihood \times prior}{\text{average likelihood}}$$

In R code form:
```{r c45}
w <- 6; n <- 9;
p_grid <- seq(from = 0, to = 1, length.out = 100)
posterior <- dbinom(w, n, p_grid) * dunif(p_grid, 0, 1)
posterior <- posterior/ sum(posterior)
plot(posterior~p_grid, type = "l")
```

$~$

## A Gaussian model of height

There are an infinite number of possible Gaussian distributions. Some have small means. Others have large means. Some are wide, with a large $\sigma$. Others are narrow. We want our Bayesian machine to consider every possible distribution, each defined by a combination of $\mu$ and $\sigma$, and rank them by posterior plausibility. Posterior plausibility provides a measure of the logical compatibility of each possible distribution with the data and model.

In practice we’ll use approximations to the formal analysis. So we won’t really consider every possible value of $\mu$ and $\sigma$.  The “estimate” here will be the entire posterior distribution, not any point within it. And as a result, the posterior distribution will be a distribution of Gaussian distributions. Yes, a distribution of distributions.

$~$

### The data

A data frame is a special kind of list in R. So you access the individual variables with the usual list “double bracket” notation, like d[[1]] for the first variable or d[['x']] for the variable named x. Unlike regular lists, however, data frames force all variables to have the same length. That isn’t always a good thing. And that’s why some statistical packages, like the powerful Stan Markov chain sampler (mc-stan.org), accept plain lists of data, rather than proper data frames.

```{r c46}

# Load the data
data("Howell1")

# name the data frame as "d"
d <- Howell1

# Inspect the structure of the data frame
str(d)

# Select the height column
 d0 <- d$height
 
# Filter the data frame down to individuals of age 18 or greater 
 d2 <- d[d$age >= 18, ]
# You can access any value in the matrix with d[row,col], replacing row and col 
# with row and column numbers. If row or col are lists of numbers, then you get
# more than one row or column. If you leave the spot for row or col blank, then 
# you get all of whatever you leave blank. For example,d[ 3 , ]gives all columns 
# at row 3. Typing d[,] just gives you the entire matrix, because it returns all 
# rows and all columns.
 
# d[ d$age >= 18 , ] gives you all of the rows in which d$age is greater-than- 
# or-equal-to 18. It also gives you all of the columns, because the spot after 
# the comma is blank.

```
$~$

### The model

Our goal is to model these values using a Gaussian distribution.

```{r c47}

# First, plot the distribution of heights
dens(d2$height)

```


Whatever the reason, adult heights are nearly always approximately normal. So it’s reasonable for the moment to adopt the stance that the model’s likelihood should be Gaussian. But be careful about choosing the Gaussian distribution only when the plotted outcome variable looks Gaussian to you. Gawking at the raw data, to try to decide how to model them, is usually not a good idea. The data could be a mixture of different Gaussian distributions, for example, and in that case you won’t be able to detect the underlying normality just by eyeballing the outcome distribution. Furthermore, the empirical distribution needn’t be actually Gaussian in order to justify using a Gaussian likelihood.

There are an infinite number of Gaussain distributions , with an infinite number of different means and standard deviations. We’re ready to write down the general model and compute the plausibility of each combination of $\mu$ and $\sigma$. To define the heights as normally distributed with a mean $\mu$ and standard deviation $\sigma$, we write:

$$\text{h}_i \sim \text{Normal}(\mu, \sigma)$$                                  

```
h             = the list of heights
subscript i   = index- each individual element of this list (row numbers)

```

$~$

Then, we add priors assuming Pr($\mu$, $\sigma$) = Pr($\mu$) Pr($\sigma$):

\begin{align*}
\text{h}_i &\sim \text{Normal}(\mu, \sigma) \ &&& [\text{Likelihood}]\\

\mu &\sim \text{Normal}(178, 20) \ &&& [\mu \text{ prior}]\\

\sigma &\sim \text{Uniform}(0, 50) \ &&& [\sigma \text{ prior}]
\end{align*}

$~$

Plot the priors to have a sense of the assumption they build into the model:

```{r c48}

curve(dnorm(x, 178, 20), from = 100, to = 250)

# golem is assuming that the average height (not each individual height) is almost certainly 
# between 140 cm and 220 cm. So this prior carries a little information, but not a lot. 
# The σ prior is a truly flat prior, a uniform one, that functions just to constrain σ 
# to have positive probability between zero and 50cm. 

```

$~$

```{r c49}

curve(dunif(x, 0, 50), from = -10, to = 60)

# A standard deviation like σ must be positive, so bounding it at zero makes sense. 
# How should we pick the upper bound? In this case, a standard deviation of 50cm would 
# imply that 95% of individual heights lie within 100cm of the average height. 
# That’s a very large range.

```

$~$

Simulate heights by sampling from priors (every posterior is also potentially a prior for a subsequent analysis, so you can process priors just like posteriors.):

```{r c50}

sample_mu <- rnorm(1e4, 178, 20)

sample_sigma <- runif(1e4, 0, 50)

prior_h <- rnorm(1e4, sample_mu, sample_sigma)

dens(prior_h)

# The density plot you get shows a vaguely bell-shaped density with thick tails. 
# It is the expected distribution of heights, averaged over the prior. Notice that 
# the prior probability distribution of height is not itself Gaussian. This is okay. 
# The distribution you see is not an empirical expectation, but rather the distribution 
# of relative plausibilities of different heights, before seeing the data.

```

$~$

### Grid approximation of the posterior distribution

The guts of the golem:

```{r c51}

# Create a grid of mu and sigma values
mu.list <- seq( from=140, to=160 , length.out=200 )
sigma.list <- seq( from=4 , to=9 , length.out=200 )
post <- expand.grid( mu=mu.list , sigma=sigma.list )

# Calculate log-likelihood
post$LL <- sapply( 1:nrow(post) , function(i) sum( dnorm( #calculate PDF of normal distribution
                d2$height , # sum to add up all log-likelihoods
                mean=post$mu[i] ,
                sd=post$sigma[i] ,
                log=TRUE ) ) ) # return log-likelihoods

# Calculate probabilities from log-likelihoods
post$prod <- post$LL + dnorm( post$mu , 178 , 20 , TRUE ) +
    dunif( post$sigma , 0 , 50 , TRUE )
# sum of all log-likelihoods + the log-likelihood of a normal distribution 
# with a fixed mean of 178 and standard deviation of 20 + he log-likelihood of 
# a uniform distribution for sigma values between 0 and 50

# Taking the sum effectively combines the contributions from the likelihood and the priors.
# This is constructing the unnormalized log-posterior distribution by summing up the 
# log-likelihood and the log-prior terms. The final step of the analysis involves 
# normalizing these probabilities to obtain a proper probability distribution.

# Use exp function to exponentiate the log-likelihoods, transforming them back to probabilities
post$prob <- exp( post$prod - max(post$prod) )
# subtracting helps with numerical stability by preventing very large or small values

# Simple contour plot
contour_xyz( post$mu , post$sigma , post$prob )

# Simple heat map
image_xyz( post$mu , post$sigma , post$prob )
```

$~$

### Sampling from the posterior

Since there are two parameters, and we want to sample combinations of them, we first randomly sample row numbers in post in proportion to the values in post$prob. Then we pull out the parameter values on those randomly sampled rows. 

```{r 4.17}

# First randomly sample row numbers in post in proportion to the values in post$prob
sample.rows <- sample( 1:nrow(post) , size=1e4 , replace=TRUE ,
    prob=post$prob )

# Pull out the parameter values on those randomly sampled rows
sample.mu <- post$mu[ sample.rows ]
sample.sigma <- post$sigma[ sample.rows ]
# You end up with 10,000 samples, with replacement, from the posterior for the height data.

# Plot the samples
plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )
# Samples from the posterior distribution for the heights data. The density of 
# points is highest in the center, reflecting the most plausible combinations of μ and σ.
# There are many more ways for these parameter values to produce the data, conditional on the model.

# Describe the distribution of confidence in each combination of μ and σ by summarizing the samples
# Characterize the shapes of the marginal posterior densities of μ and σ
# The jargon “marginal” here means “averaging over the other parameters.” 
dens(sample.mu, main = "sample.mu")
dens(sample.sigma, main = "sample.sigma")
# As sample size increases, posterior densities approach the normal distribution. 
# If you look closely, though, you’ll notice that the density for σ has a longer right-hand tail.
# This condition is very common for standard deviation parameters.

# Summarize the widths of these densities with highest posterior density intervals
HPDI(sample.mu)
HPDI(sample.sigma)
# Since these samples are just vectors of numbers, you can compute any statistic 
# from them that you could from ordinary data. If you want the mean or median, 
# just use the corresponding R functions.

```

$~$

<span style="color:brown"><span style="text-decoration:underline">**Sample size and the normality of $\sigma$'s posterior**</span>

- Repeat the above analysis using only a fraction of the original data to demonstrate the posterior is not always so Gaussian in shape

- For a Gaussian likelihood and a Gaussian prior on μ, the posterior distribution is always Gaussian as well, regardless of sample size.

- There’s no trouble with the mean, μ. It is the standard deviation σ that causes problems- need to be careful of abusing the quadratic approximation.

- The deep reasons for the posterior of σ tending to have a long right-hand tail are complex. But a useful way to conceive of the problem is that variances must be positive. As a result, there must be more uncertainty about how big the variance (or standard deviation) is than about how small it is. For example, if the variance is estimated to be near zero, then you know for sure that it can’t be much smaller. But it could be a lot bigger.

```{r 4.21}

# Take only 20 samples from height data
d3 <- sample(d2$height, size = 20)

# Create a grid from mean and SD values
mu.list <- seq( from=150, to=170 , length.out=200 )
sigma.list <- seq( from=4 , to=20 , length.out=200 )
post2 <- expand.grid( mu=mu.list , sigma=sigma.list )

# Calculate log-likelihoods
post2$LL <- sapply( 1:nrow(post2) , function(i)
    sum( dnorm( d3 , mean=post2$mu[i] , sd=post2$sigma[i] ,
    log=TRUE ) ) )

# Calculate probabilities from log-likelihood values
post2$prod <- post2$LL + dnorm( post2$mu , 178 , 20 , TRUE ) +
    dunif( post2$sigma , 0 , 50 , TRUE )

# Exponentiate the log-likelihoods back to probability
post2$prob <- exp( post2$prod - max(post2$prod) )

# Randomly sample row numbers in post2 in proportion to probability
sample2.rows <- sample( 1:nrow(post2) , size=1e4 , replace=TRUE ,
    prob=post2$prob )

# Draw the parameter values from randomly sampled row numbers
sample2.mu <- post2$mu[ sample2.rows ]
sample2.sigma <- post2$sigma[ sample2.rows ]

# Plot the samples
plot( sample2.mu , sample2.sigma , cex=0.5 ,
    col=col.alpha(rangi2,0.1) ,
    xlab="mu" , ylab="sigma" , pch=16 )
# Notice a distinctly longer tail at the top of the cloud of points

# Inspect the marginal posterior density for σ, averaging over μ
dens( sample2.sigma , norm.comp=TRUE, col = "purple" )
# Now you can see that the posterior for σ is not Gaussian, but rather has a long tail of
# uncertainty towards higher values.

```

$~$

### Fitting the model with `map`

- Quadratic approximation is as a handy way to quickly make inferences about the shape of the posterior.

- The posterior’s peak will lie at the maximum a posteriori estimate (MAP), and we can get a useful image of the posterior’s shape by using the quadratic approximation of the posterior distribution at this peak.

$~$

<span style="color:brown; text-decoration:underline">**Use `map` to find the values of $\mu$ and $\sigma$ that maximize the posterior probability:**</span>

```{r 4.24}

# Load Howell1 data and select out the adults
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18 , ]

```

- Define the model, using R's formula syntax:

\begin{align*}
\text{h}_i & \sim \text{Normal}(\mu, \sigma) \ &&& \text{[height} & \sim \text{dnorm(mu, sigma)}\text{]}\\

\mu &\sim \text{Normal}(178, 20) \ &&& [\text{mu} &\sim \text{dnorm}(178, 20)]\\

\sigma &\sim \text{Uniform}(0, 50) \ &&& [\text{sigma} &\sim \text{dunif}(0, 50)]
\end{align*}

$~$

- Now, place the R code equivalents into `alist`. 

```{r 4.25}

flist <- alist(
    height ~ dnorm( mu , sigma ) ,
    mu ~ dnorm( 178 , 20 ) ,
    sigma ~ dunif( 0 , 50 )
)
# Note the commas at the end of each line, except the last. 
# These commas separate each line of the model definition.
```

$~$

- Fit the model to the data in the data frame d2 with: 

```{r 4.26}

m4.1 <- map( flist , data=d2 )

# After executing this code, you’ll have a fit model stored in the symbol m4.1.

```

$~$

- Now take a look at the fit maximum a posteriori model:

```{r 4.27}

precis(m4.1)

# These numbers provide Gaussian approximations for each parameter’s marginal distribution. 
# This means the plausibility of each value of μ, after averaging over the plausibilities 
# of each value of σ, is given by a Gaussian distribution with mean 154.6 and standard deviation 0.4.
# The 5.5% and 94.5% quantiles are percentile interval boundaries, corresponding to an 89% interval.
# They are almost identical with the HPDIs from the grid approximation earlier.
# When the posterior is approximately Gaussian, then this is what you should expect.

```

$~$

<span style="color:brown; text-decoration:underline">**Start values for `map`**</span>

`map` estimates the posterior by climbing it like a hill. To do this, it has to start climbing someplace, at some combination of parameter values. Unless you tell it otherwise, map starts at random values sampled from the prior. But it’s also possible to specify a starting value for any parameter in the model.

```{r 4.28}

# Use parameters μ and σ as starting values- good guesses of the rough location of the MAP values
start <- list(
    mu=mean(d2$height),
    sigma=sd(d2$height)
)

```

```
Note that the list of start values is a regular list, not an alist like the formula list is. 
The two functions alist and list do the same basic thing: allow you to make a collection of arbitrary R objects. They differ in one important respect: list evaluates the code you embed inside it, while
alist does not. So when you define a list of formulas, you should use alist, so the code isn’t 
executed. But when you define a list of start values for parameters, you should use list, so 
that code like mean(d2$height) will be evaluated to a numeric value.

```

$~$

See the effect of a more information prior for $\mu$:

```{r 4.29}

# Change the standard deviation of the prior to 0.1, so it’s a very narrow prior
m4.2 <- map(
        alist(
            height ~ dnorm( mu , sigma ) ,
            mu ~ dnorm( 178 , 0.1 ) ,
            sigma ~ dunif( 0 , 50 )
),
        data=d2 )

precis( m4.2 )

# Notice that the estimate for μ has hardly moved off the prior. 
# The prior was very concentrated around 178. Also notice that the estimate for 
# σ has changed quite a lot, even though we didn’t change its prior at all.
# Once the golem is certain that the mean is near 178—as the prior insists—then 
# the golem has to estimate σ conditional on that fact. This results in a different
# posterior for σ, even though all we changed is prior information about the other parameter.

```

$~$

### Sampling from a `map` fit

Getting samples from the quadratic approximate posterior distribution requires recognizing that a quadratic approximation to a posterior distribution with more than one parameter dimension— μ and σ each contribute one dimension— is just a multi-dimensional Gaussian distribution.

As a consequence, when R constructs a quadratic approximation, it calculates not only standard deviations for all parameters, but also the covariances among all pairs of parameters.

```{r 4.30}

#  A multi-dimensional Gaussian distribution requires a list of means and 
# a matrix of variances and covariances
vcov( m4.1 )

# The above is a variance-covariance matrix- it tells us how each parameter relates 
# to every other param- eter in the posterior distribution.

```

A variance-covariance matrix can be factored into two elements: 

(1) a vector of variances for the parameters 

(2) a correlation matrix that tells us how changes in any parameter lead to correlated changes in the others.

```{r 4.31}

diag( vcov( m4.1 ) )
# The two-element vector in the output is the list of variances. 
# If you take the square root of this vector, you get the standard deviations that 
# are shown in precis output.

cov2cor( vcov( m4.1 ) )
# The two-by-two matrix in the output is the correlation matrix. Each entry shows 
# the correlation, bounded between −1 and +1, for each pair of parameters. The 1’s 
# indicate a parameter’s correlation with itself. If these values were anything 
# except 1, we would be worried. The other entries are typically closer to zero, 
# and they are very close to zero in this example. This indicates that learning 
# μ tells us nothing about σ and likewise that learning σ tells us nothing about μ.

```
$~$

To get samples from this multi-dimensional posterior, we sample vectors of values from a multi-dimensional Gaussian distribution- instead of sampling single values from a simple Gaussian distribution.

```{r 4.32}

post <- extract.samples( m4.1 , n=1e4 )
head(post)
# You end up with a data frame, post, with 10,000 (1e4) rows and two columns, 
# one column for μ and one for σ. Each value is a sample from the posterior, 
# so the mean and standard deviation of each column will be very close to the
# MAP values from before.

# Confirm this by summarizing the samples
precis(post)

# Compare with values from quadratic approximate posterior distribution
precis(m4.1)

# Compare with the samples from the grid approximation
par(mfrow = c(1, 2))
plot(post, cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1))
plot( sample.mu , sample.sigma , cex=0.5 , pch=16 , col=col.alpha(rangi2,0.1) )
# There is resembalance with the grid approximation
# These samples also preserve the covariance between μ and σ. This hardly matters right now, 
# because μ and σ don’t covary at all in this model. But once you add a predictor variable 
# to your model, covariance will matter a lot.

```

$~$

<span style="color:purple">***Under the hood with multivariate sampling***</span>

The function extract.samples is for convenience. The work is done by a multi-dimensional version of rnorm, mvrnorm. The function rnorm simulates random Gaussian values, while mvrnorm simulates random vectors of multivariate Gaussian values. Here’s how to use it directly to do what extract.samples does:

```{r 4.34}

# library(MASS)
post <- mvrnorm( n=1e4 , mu=coef(m4.1) , Sigma=vcov(m4.1) )

# You don’t usually need to use mvrnorm directly like this, but sometimes you want to
# simulate multi-variate Gaussian outcomes. In that case, you’ll need to access mvrnorm directly.

```

$~$

<span style="color:purple">***Getting $\sigma$ right***</span>

- The quadratic assumption for σ can be problematic- more uncertainty at higher values. 

- A conventional way to improve the situation is the estimate log(σ) instead- while the posterior distribution of σ will often not be Gaussian, the distribution of its logarithm can be much closer to Gaussian.

- So if we impose the quadratic approximation on the logarithm, rather than the standard deviation itself, we can often get a better approximation of the uncertainty.

```{r 4.35}
set.seed(435)
# Estimate log(σ) using map
m4.1_logsigma <- map(
        alist(
          # the exp inside the likelihood converts a continuous parameter, log_sigma, 
          # to be strictly positive, because exp(x) > 0 for any real value x.
            height ~ dnorm( mu , exp(log_sigma) ) ,
            mu ~ dnorm( 178 , 20 ) ,
            log_sigma ~ dnorm( 2 , 10 )   
            # Since log_sigma is continuous now, it can have a Gaussian prior.
) , data=d2 )

# When you extract samples, it is log_sigma that has a Gaussian distribution. 
# To get the distribution of sigma, you just need to use the same exp as in the 
# model definition, to get back on the natural scale:
post <- extract.samples( m4.1_logsigma )
sigma <- exp( post$log_sigma )

# Compare the three samples of sigma
par(mfrow = c(1, 3))
dens(sigma, main ="log.sigma")
dens(sample.sigma, main ="sample.sigma")
dens( sample2.sigma , main = "smaller.sample.sigma")
# log.sima and sample.sigma have same sample sizes, while smaller.sample.sigma has
# significantly smaller sample size of 20. 
# When you have a lot of data, this won’t make any noticeable difference. 
# But the use of exp to effectively constrain a parameter to be positive is a 
# robust and useful one. And it relates to link functions.

```

$~$

## Adding a predictor

Typically, we are interested in modeling how an outcome is related to some predictor variable. And by including a predictor variable in a particular way, we’ll have linear regression.

```{r c4.37}

# See how height covaries with weight
plot(d2$height ~ d2$weight)

# there’s obviously a relationship: Knowing a person’s weight helps you predict height.
# Incorporate predictor variables to Gaussian model to make this vague observation into
# a more precise quantitative model that relates values of weight to plausible values of height

```

$~$

## Fitting the model

All we have to do is incorporate our new model for the mean into the model specification inside map and be sure to add our new parameters to the start list.

Repeat model definition, with corresponding R code:

\begin{align*}
\text{h}_i & \sim \text{Normal}(\mu_i, \sigma) \ &&& \text{height} & \sim \text{dnorm(mu, sigma)} \\

\mu_i & = \alpha + \beta x_i \ &&& \text{mu} & \leftarrow \text{a + b*weight} \\

\alpha &\sim \text{Normal}(178, 100) \ &&& a &\sim \text{dnorm}(178, 100)\\

\beta &\sim \text{Normal}(0, 10) \ &&& b &\sim \text{dnorm}(0, 10)\\

\sigma &\sim \text{Uniform}(0, 50) \ &&& \text{sigma} &\sim \text{dunif}(0, 50)

\end{align*}

$~$

Build the MAP model fit using the above formulas:

```{r 4.38}

# Load data Howell1
data(Howell1)
d <- Howell1
d2 <- d[d$age >= 18, ]

# fit model
m4.3 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*weight,
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)

# Note that starting b at zero is not the same as having β’s prior with mean zero. 
# The values in the start list don’t alter the posterior probabilities, while priors definitely do.

precis(m4.3)

# Another way to fit the same model without a separate line for linear model
# Merge the linear model into likelihood function
m4.3 <- map(
  alist(
    height ~ dnorm(a + b*weight, sigma),
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)

precis(m4.3)




```

$~$

## Interpreting the model fit

Two broad categories of processing:

(1) reading tables

(2) plotting

Plotting the implications of your estimates will allow you to inquire about several things that are sometimes hard to read from tables:

- Whether or not the model fitting procedure worked correctly

- The absolute magnitude, rather than merely relative magnitude, of a relationship between outcome and predictor

- The uncertainty surrounding an average relationship

- The uncertainty surrounding the implied predictions of the model, as these are distinct from mere parameter uncertainty

$~$

<span style="color:purple">***What do parameters mean?***</span>

Posterior probabilities of parameter values describe the relative compatibility of different states of the world with the data, ac- cording to the model.

$~$

### Tables of estimates

With the new linear regression fit to the Kalahari data, we inspect the estimates:

```{r 4.40}

precis(m4.3)

# The first row gives the quadratic approximation for α, the second the approximation 
# for β, and the third approximation for σ.
# Since β is a slope, the value 0.90 can be read as a person 1 kg heavier is expected 
# to be 0.90 cm taller. 89% of the posterior probability lies between 0.84 and 0.97. 
# The estimate of α, a in the precis table, indicates that a person of weight 0 should be 114cm tall.
# The estimate for σ, sigma, informs us of the width of the distribution of heights around the mean.
# The estimate tells us that 95% of plausible heights lie within 10cm (2σ) of the mean height. 
# But there is also uncertainty about this, as indicated by the 89% percentile interval

```

$~$

The numbers in the default precis output aren’t sufficient to describe the quadratic posterior completely. For that, we also require the variance-covariance matrix. We’re interested in correlations among parameters—we already have their variance in the table above—so let’s go straight to the correlation matrix:

```{r 4.41}

# Correlation matrix
precis( m4.3 , corr=TRUE )# This code not working

cov2cor(vcov(m4.3))

# Notice that α and β are almost perfectly negatively correlated. Right now, 
# this is harmless. It just means that these two parameters carry the same information—
# as you change the slope of the line, the best intercept changes to match it. 
# But in more complex models, strong correlations like this can make it difficult 
# to fit the model to the data. One of the tricks to avoid it is centering.
```

$~$

<span style="color:brown"><span style="text-decoration:underline">**Centering**</span>

Centering is the procedure of subtracting the mean of a variable from each value. 

```{r 4.42}

# Create a centered version of the weight variable
d2$weight.c <- d2$weight - mean(d2$weight)

# Confirm that the average value of weight.c is zero
mean(d2$weight.c)

# Refit the model and see the changes

m4.4 <- map(
    alist(
        height ~ dnorm( mu , sigma ) ,
        mu <- a + b*weight.c ,
        a ~ dnorm( 178 , 100 ) ,
        b ~ dnorm( 0 , 10 ) ,
sigma ~ dunif( 0 , 50 ) ),
data=d2,
start = list(a = 178, b = 0, sigma = 1))
# I got error "Start values for parameters may be too far from MAP." if specify start value.

precis(m4.4, corr = TRUE) 
cov2cor(vcov(m4.4))

# The estimates for β and σ are unchanged (within rounding error), but the estimate for
# α (a) is now the same as the average height value in the raw data. 
# And the correlations among parameters are now all zero
mean(d2$height)

# The estimate for the intercept, α, still means the same thing it did before: 
# the expected value of the outcome variable, when the predictor variable is equal to zero. 
# But now the mean value of the predictor is also zero. So the intercept also means: 
# the expected value of the outcome, when the predictor is at its average value. 
# This makes interpreting the intercept a lot easier.

```

$~$

### Plotting posterior inference aganist the data

Not only does plotting help in interpreting the posterior, but it also provides an informal check on model assumptions. When the model’s predictions don’t come close to key observations or patterns in the plotted data, then you might suspect the model either did not fit correctly or is rather badly specified.

We’re going to start with a simple version of that task, superimposing just the MAP values over the height and weight data.

```{r 4.45}

# superimpose the MAP values for mean height over the actual data
plot(height ~ weight, data = d2, col = "blue")
abline(a = coef(m4.3)["a"], b = coef(m4.3)["b"])
# Height in centimeters (vertical) plotted against weight in kilograms (horizontal), 
# with the maximum a posteriori (MAP) line for the mean height at each weight plotted in black.

```

$~$

### Adding uncertainty around the mean

The posterior distribution considers every possible regression line connecting height to weight. It assigns a relative plausibility to each. This means that each combination of α and β has a posterior probability. It could be that there are many lines with nearly the same posterior probability as the MAP line. Or it could be instead that the posterior distribution is rather narrow near the MAP line.

So how can we get that uncertainty onto the plot? Together, a combination of α and β define a line. And so we could sample a bunch of lines from the posterior distribution. Then we could display those lines on the plot, to visualize the uncertainty in the regression relationship.

```{r 4.46}

# To better appreciate how the posterior distribution contains lines, extract some samples from the model
post <- extract.samples(m4.3)

# Inspect the first 5 rows of the samples
post[1:5,]

```

$~$

Each row is a correlated random sample from the joint posterior of all three parameters, using the covariances provided by vcov(m4.3). The paired values of a and b on each row define a line. The average of very many of these lines is the MAP line. But the scatter around that average is meaningful, because it alters our confidence in the relationship between the predictor and the outcome.

```{r 4.48}

# extracts the first 10 cases and re-estimates the model
N <- 10
dN <- d2[1:N, ]
mN <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*weight,
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ), data = dN
)

# plot 20 of these lines, to see what the uncertainty looks like
# extract 20 samples from the posterior
post <- extract.samples(mN, n = 20)

# display raw data and sample size
plot(dN$weight, dN$height,
     xlim = range(d2$weight), ylim = range(d2$height),
     col = rangi2, xlab = "weight", ylab = "height")
mtext(concat("N = ", N)) # add text to the plot indicating sample size

# plot the lines with transparency
# i-th sample is a set of random parameter values sampled from posterior distribution
for (i in 1:20) # loops over all 20 lines, using abline to display each
  abline(a = post$a[i], b = post$b[i], col = col.alpha("black", 0.3))

# The cloud of regression lines displays greater uncertainty at extreme values for weight. 
# This is very common.

```

$~$

```{r 4.48.1}

# compare higher N values (bigger sample size)

par(mfrow = c(2,2))

# 20 samples
## show error in knitting if I use 10 sample here, although it works above with warning
## 10 samples is too low to reach stable estimates
N <- 20
dN <- d2[1:N, ]
mN <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b*weight,
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ), data = dN
)

post <- extract.samples(mN, n = 20)

plot(dN$weight, dN$height,
     xlim = range(d2$weight), ylim = range(d2$height),
     col = rangi2, xlab = "weight", ylab = "height")
mtext(concat("N = ", N)) 

for (i in 1:20) 
  abline(a = post$a[i], b = post$b[i], col = col.alpha("black", 0.3))


# 50 samples
N1 = 50
dN1 <- d2[1:N1, ]
mN1 <- map(
  alist(
    height ~ dnorm( mu , sigma ) ,
        mu <- a + b*weight ,
        a ~ dnorm( 178 , 100 ) ,
        b ~ dnorm( 0 , 10 ) ,
        sigma ~ dunif( 0 , 50 )
  ), data = dN1
)

post1 <- extract.samples( mN1, n=20)

plot( dN1$weight , dN1$height ,
    xlim=range(d2$weight) , ylim=range(d2$height) ,
    col=rangi2 , xlab="weight" , ylab="height" )
mtext(concat("N = ",N1))

for ( i in 1:20 )
    abline( a=post1$a[i] , b=post1$b[i] , col=col.alpha("black",0.3) )

# 150 samples
N2 = 150
dN2 <- d2[1:N2, ]
mN2 <- map(
  alist(
    height ~ dnorm( mu , sigma ) ,
        mu <- a + b*weight ,
        a ~ dnorm( 178 , 100 ) ,
        b ~ dnorm( 0 , 10 ) ,
        sigma ~ dunif( 0 , 50 )
  ), data = dN2
)

post2 <- extract.samples( mN2, n=20 )

plot( dN2$weight , dN2$height ,
    xlim=range(d2$weight) , ylim=range(d2$height) ,
    col=rangi2 , xlab="weight" , ylab="height" )
mtext(concat("N = ",N2))

for ( i in 1:20 )
    abline( a=post2$a[i] , b=post2$b[i] , col=col.alpha("black",0.3) )

# 352 samples
N3 = 352
dN3 <- d2[1:N3, ]
mN3 <- map(
  alist(
    height ~ dnorm( mu , sigma ) ,
        mu <- a + b*weight ,
        a ~ dnorm( 178 , 100 ) ,
        b ~ dnorm( 0 , 10 ) ,
        sigma ~ dunif( 0 , 50 )
  ), data = dN3
)

post3 <- extract.samples( mN3, n=20 )

plot( dN3$weight , dN3$height ,
    xlim=range(d2$weight) , ylim=range(d2$height) ,
    col=rangi2 , xlab="weight" , ylab="height" )
mtext(concat("N = ",N3))

for ( i in 1:20 )
    abline( a=post3$a[i] , b=post3$b[i] , col=col.alpha("black",0.3) )

# The cloud of regression lines grows more compact as the sample size increases. 
# This is a result of the model growing more confident about the location of the mean.

```

$~$

### Plotting regression intervals and contours



















                      
                      









---
